---
phase: 04.7-dashboard-enhancements
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - python-src/extractors/llm/client.py
  - python-src/extractors/llm/openai_client.py
  - python-src/main.py
autonomous: true

must_haves:
  truths:
    - "Ollama client captures prompt_eval_count and eval_count from response"
    - "OpenAI client captures usage.prompt_tokens and usage.completion_tokens from response"
    - "Python extraction response includes token_usage object"
  artifacts:
    - path: "python-src/main.py"
      provides: "Token usage in extraction response"
      contains: "token_usage"
    - path: "python-src/extractors/llm/client.py"
      provides: "Ollama token capture"
      contains: "prompt_eval_count"
  key_links:
    - from: "client.py"
      to: "main.py"
      via: "Return value with token_usage"
      pattern: "token_usage"
---

<objective>
Modify Python LLM clients to capture and return token usage from extraction responses.

Purpose: Enables tracking of actual token consumption for usage limits and cost visibility.
Output: Python responses include token_usage field with prompt_tokens, completion_tokens, total_tokens.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.7-dashboard-enhancements/04.7-RESEARCH.md
@python-src/extractors/llm/client.py
@python-src/extractors/llm/openai_client.py
@python-src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Capture token counts in Ollama client</name>
  <files>python-src/extractors/llm/client.py</files>
  <action>
    Modify the OllamaClient class to capture and return token usage.

    In the extract method (or the method that calls self._client.chat), after getting the response:

    1. Extract token counts from Ollama response:
       - prompt_tokens = response.prompt_eval_count or 0
       - completion_tokens = response.eval_count or 0
       - total_tokens = prompt_tokens + completion_tokens

    2. Return token_usage alongside the parsed result. If the method currently returns just the parsed data, change it to return a tuple or dict that includes both.

    For example, if the method signature is:
    ```python
    def extract_full(self, text: str) -> Optional[LLMFullExtraction]:
    ```

    Change the internal logic to track token usage and return it. The cleanest approach:
    - Add an instance variable self._last_token_usage that gets set after each call
    - Add a method get_last_token_usage() -> dict that returns the last token usage
    - OR return a tuple (result, token_usage) and update callers

    The preferred approach is to store as instance variable since it minimizes changes:
    ```python
    self._last_token_usage = {
        'prompt_tokens': response.prompt_eval_count or 0,
        'completion_tokens': response.eval_count or 0,
        'total_tokens': (response.prompt_eval_count or 0) + (response.eval_count or 0),
    }
    ```

    Add a getter method:
    ```python
    def get_last_token_usage(self) -> dict:
        """Return token usage from the last extraction call."""
        return self._last_token_usage or {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
    ```

    Initialize self._last_token_usage = None in __init__.
  </action>
  <verify>python -m py_compile python-src/extractors/llm/client.py shows no syntax errors</verify>
  <done>OllamaClient stores and exposes token usage from last extraction</done>
</task>

<task type="auto">
  <name>Task 2: Capture token counts in OpenAI client</name>
  <files>python-src/extractors/llm/openai_client.py</files>
  <action>
    Modify the OpenAIClient class to capture and return token usage.

    In the extract method (or the method that calls the OpenAI API), after getting the response:

    1. Extract token counts from OpenAI response:
       - The response object has a .usage attribute with:
         - response.usage.prompt_tokens
         - response.usage.completion_tokens
         - response.usage.total_tokens

    2. Store as instance variable (same pattern as Ollama client):
       ```python
       self._last_token_usage = {
           'prompt_tokens': response.usage.prompt_tokens,
           'completion_tokens': response.usage.completion_tokens,
           'total_tokens': response.usage.total_tokens,
       }
       ```

    3. Add the same getter method:
       ```python
       def get_last_token_usage(self) -> dict:
           """Return token usage from the last extraction call."""
           return self._last_token_usage or {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
       ```

    4. Initialize self._last_token_usage = None in __init__.

    Note: If using beta.chat.completions.parse, the response still has usage - check OpenAI docs for exact attribute path.
  </action>
  <verify>python -m py_compile python-src/extractors/llm/openai_client.py shows no syntax errors</verify>
  <done>OpenAIClient stores and exposes token usage from last extraction</done>
</task>

<task type="auto">
  <name>Task 3: Include token usage in extraction response</name>
  <files>python-src/main.py</files>
  <action>
    Modify the extraction response in main.py to include token_usage.

    Find the code that handles CV extraction (likely a function or endpoint that:
    1. Gets the LLM client (OllamaClient or OpenAIClient based on mode)
    2. Calls extraction
    3. Returns JSON response

    After successful extraction, before returning the response:

    1. Get token usage from the client:
       ```python
       token_usage = client.get_last_token_usage()
       ```

    2. Include in the response data:
       ```python
       return {
           'id': request_id,
           'success': True,
           'data': {
               **parsed_cv_dict,  # existing CV data
               'token_usage': token_usage,  # ADD THIS
           }
       }
       ```

    Also apply the same pattern to JD extraction if it uses LLM.

    Make sure token_usage is included even when extraction fails (with zeros):
    ```python
    'token_usage': client.get_last_token_usage() if client else {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
    ```
  </action>
  <verify>python -m py_compile python-src/main.py shows no syntax errors</verify>
  <done>Extraction responses include token_usage field</done>
</task>

</tasks>

<verification>
After all tasks:
1. Run `python -m py_compile python-src/main.py` - no errors
2. Run `python -m py_compile python-src/extractors/llm/client.py` - no errors
3. Run `python -m py_compile python-src/extractors/llm/openai_client.py` - no errors
4. If tests exist: `cd python-src && python -m pytest` passes
</verification>

<success_criteria>
- Both LLM clients have get_last_token_usage() method
- Extraction responses include token_usage object
- Token counts are captured correctly (non-zero for actual extractions)
- No Python syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/04.7-dashboard-enhancements/04.7-02-SUMMARY.md`
</output>
