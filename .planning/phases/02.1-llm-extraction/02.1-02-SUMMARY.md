---
phase: 02.1-llm-extraction
plan: 02
subsystem: extraction
tags: [llm, unified-extraction, hybrid, ollama, performance]

# Dependency graph
requires:
  - phase: 02.1-01
    provides: OllamaClient, Pydantic schemas, extraction prompts
provides:
  - Unified single-call LLM extraction (1 call instead of 4)
  - Full raw_text extraction handles multi-column PDFs
  - Graceful regex fallback when Ollama unavailable
affects: [03-visual-editor, 04-jd-matching]

# Tech tracking
tech-stack:
  added: []
  patterns: [unified-llm-extraction, single-call-optimization]

key-files:
  created: []
  modified:
    - python-src/main.py
    - python-src/extractors/llm/schemas.py
    - python-src/extractors/llm/prompts.py
    - python-src/extractors/llm/__init__.py
    - src/main/pythonManager.ts
    - src/renderer/renderer.ts

key-decisions:
  - "Single unified LLM call instead of 4 separate calls - faster and more accurate"
  - "Pass full raw_text to LLM - handles multi-column PDF layouts better than section detection"
  - "120s timeout for unified extraction (complex schema needs more time)"
  - "Cost efficiency: 1 API call always preferred over multiple calls"

patterns-established:
  - "Unified extraction: One LLM call extracts contact, work_history, education, skills"
  - "Full context: LLM sees entire CV text for better understanding"
  - "Graceful fallback: If LLM fails, regex extractors handle each field"

# Metrics
duration: 45min (including debugging and user verification)
completed: 2026-01-25
---

# Phase 2.1 Plan 02: Hybrid Extraction Pipeline Summary

**Unified single-call LLM extraction with full CV context for accurate parsing**

## Performance

- **Duration:** ~45 min (with user testing/iteration)
- **Started:** 2026-01-25T12:00:00Z
- **Completed:** 2026-01-25T14:30:00Z
- **Tasks:** 3 (including human verification checkpoint)
- **Files modified:** 6

## Accomplishments
- Replaced 4 separate LLM calls with 1 unified call
- LLM now receives full raw_text for better context understanding
- Successfully extracts from multi-column PDF layouts
- Skills extraction correctly identifies skills vs job descriptions
- Work history finds all jobs even when mixed with other sections

## Key Changes

**Unified Schema (LLMFullExtraction):**
- Single Pydantic model containing contact, work_history, education, skills
- Complex nested schema handled by Qwen 2.5 7B

**Unified Prompt (FULL_EXTRACTION_PROMPT):**
- Comprehensive prompt guiding LLM to extract all CV sections
- Explicit examples of what ARE vs are NOT skills
- Handles multi-column layout confusion

**main.py Changes:**
- Single `llm_client.extract()` call with LLMFullExtraction schema
- Converts LLM result to existing TypedDict formats
- Falls back to regex extractors if LLM fails

## Decisions Made
- **Single call over multiple:** 1 LLM call is always preferred for cost/speed efficiency
- **Full raw_text:** Pass entire CV text instead of detected sections
- **120s timeout:** Complex unified schema needs more processing time
- **Regex fallback:** Maintains functionality when Ollama unavailable

## Deviations from Original Plan

**Major change:** Original plan had 4 separate hybrid extraction functions (one per field). Changed to unified single-call approach based on user feedback about:
1. Cost efficiency (1 call vs 4)
2. Speed (single call is faster)
3. Accuracy (full context helps LLM understand CV structure)

## Issues Encountered
- **Initial timeout:** 60s wasn't enough for unified extraction, increased to 120s
- **Skills confusion:** LLM initially extracted job descriptions as skills, fixed with clearer prompt
- **Multi-column PDF:** Section detection failed on complex layouts, solved by using full raw_text

## Known Limitations / Future Work

**Performance concern noted by user:**
- Current extraction time: ~50-55 seconds
- Target: Should be faster
- Potential optimizations:
  - Smaller/faster model
  - Simplified schema
  - Parallel processing
  - Model quantization

## Test Results

**With user's multi-column CV:**
- Contact: Correct (name, email, phone)
- Work History: 3 jobs found (was only 1 before)
- Education: 3 entries correct
- Skills: Correct skills only (no job descriptions)

## User Setup Required

Ollama must be installed with qwen2.5:7b model:
```
ollama pull qwen2.5:7b
```

Extraction works without Ollama (regex fallback) but quality is reduced.

---
*Phase: 02.1-llm-extraction*
*Completed: 2026-01-25*
