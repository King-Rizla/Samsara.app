---
phase: 02-parsing-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - python-src/extractors/__init__.py
  - python-src/extractors/contact.py
  - python-src/extractors/sections.py
  - python-src/extractors/work_history.py
  - python-src/extractors/education.py
  - python-src/extractors/skills.py
  - python-src/normalizers/__init__.py
  - python-src/normalizers/dates.py
  - python-src/normalizers/text.py
  - python-src/schema/__init__.py
  - python-src/schema/cv_schema.py
  - python-src/main.py
autonomous: true

must_haves:
  truths:
    - "Name is extracted from CV using spaCy NER with PERSON entity"
    - "Email addresses are extracted using regex pattern"
    - "Phone numbers are extracted with UK format support"
    - "URLs (LinkedIn, GitHub) are extracted as contact fields"
    - "Work history entries have company, title, dates, and description"
    - "Education entries have institution, degree, field, and dates"
    - "Skills preserve candidate's own groupings/headings"
    - "Dates are normalized to dd/mm/yyyy British format"
    - "Confidence scores are calculated based on extraction agreement"
  artifacts:
    - path: "python-src/extractors/contact.py"
      provides: "Contact field extraction (name, email, phone, URL)"
      exports: ["extract_contacts"]
    - path: "python-src/extractors/work_history.py"
      provides: "Work history extraction with dates"
      exports: ["extract_work_history"]
    - path: "python-src/extractors/education.py"
      provides: "Education extraction with dates"
      exports: ["extract_education"]
    - path: "python-src/extractors/skills.py"
      provides: "Skills extraction preserving groupings"
      exports: ["extract_skills"]
    - path: "python-src/schema/cv_schema.py"
      provides: "TypedDict definitions for CV data structure"
      exports: ["ParsedCV", "ContactInfo", "WorkEntry", "EducationEntry"]
  key_links:
    - from: "python-src/extractors/contact.py"
      to: "spaCy nlp model"
      via: "nlp(text) for PERSON entity"
      pattern: "nlp\\(.*\\)"
    - from: "python-src/normalizers/dates.py"
      to: "dateutil.parser"
      via: "parse with dayfirst=True"
      pattern: "parse.*dayfirst.*True"
---

<objective>
Implement entity extraction from parsed CV text with confidence scoring.

Purpose: Extract structured data (contact info, work history, education, skills) from raw CV text. Use spaCy NER for names, regex for structured patterns (email, phone, URL), and section detection for splitting the CV. Calculate confidence scores based on extraction agreement, not model scores (spaCy NER doesn't expose confidence).

Output: Extractor modules that take raw text and return structured CV data with confidence scores. TypedDict schema for CV data structure.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-parsing-pipeline/02-RESEARCH.md
@.planning/phases/02-parsing-pipeline/02-CONTEXT.md
@.planning/phases/02-parsing-pipeline/02-01-SUMMARY.md
@python-src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CV schema and normalization utilities</name>
  <files>python-src/schema/__init__.py, python-src/schema/cv_schema.py, python-src/normalizers/__init__.py, python-src/normalizers/dates.py, python-src/normalizers/text.py</files>
  <action>
    **schema/cv_schema.py:** Create TypedDict definitions per RESEARCH.md:
    - ContactInfo: name, email, phone, address, linkedin, github, portfolio (all Optional[str])
    - WorkEntry: company, position, start_date, end_date, description, highlights (List[str]), confidence (float)
    - EducationEntry: institution, degree, field_of_study, start_date, end_date, grade (Optional[str]), confidence
    - SkillGroup: category (str), skills (List[str])
    - ParsedCV: contact (ContactInfo), work_history (List[WorkEntry]), education (List[EducationEntry]), skills (List[SkillGroup]), certifications (List[str]), languages (List[str]), other_sections (dict), raw_text (str), section_order (List[str]), parse_confidence (float), warnings (List[str])

    **normalizers/dates.py:**
    - normalize_date(date_str) -> Optional[str]: Use dateutil.parser.parse with dayfirst=True for British format
    - Handle "Present", "Current", "Now" -> "Present"
    - Handle en/em dashes by converting to hyphens
    - Return original string if unparseable
    - Output format: dd/mm/yyyy

    **normalizers/text.py:**
    - normalize_text(text) -> str: Unicode normalization with unicodedata.normalize('NFKC', text)
    - clean_whitespace(text) -> str: Collapse multiple spaces/newlines
    - extract_lines(text) -> List[str]: Split into lines, strip each

    Create __init__.py files with appropriate exports.
  </action>
  <verify>
    Test normalize_date with:
    - "2020-01-15" -> "15/01/2020"
    - "January 2020" -> "01/01/2020"
    - "Jan 2020" -> "01/01/2020"
    - "3/2/2020" -> "03/02/2020" (3rd Feb, not March 2nd due to dayfirst=True)
    - "Present" -> "Present"
    - "Current" -> "Present"
  </verify>
  <done>TypedDict schema defined, date normalization returns British format, text normalization handles Unicode</done>
</task>

<task type="auto">
  <name>Task 2: Implement contact and section extraction</name>
  <files>python-src/extractors/__init__.py, python-src/extractors/contact.py, python-src/extractors/sections.py</files>
  <action>
    **extractors/contact.py:**
    - EMAIL_PATTERN: regex for email addresses (case-insensitive)
    - PHONE_PATTERN: UK format (+44, 0) and international numbers
    - URL_PATTERN: LinkedIn, GitHub, portfolio URLs
    - extract_contacts(text: str, nlp) -> Tuple[ContactInfo, float]:
      1. Extract emails, phones, URLs with regex
      2. Use spaCy NER on first 2000 chars for PERSON entity (name)
      3. Use spaCy NER for GPE entity (location/address - first match only)
      4. Post-process: If PERSON contains company indicators (Ltd, Inc, LLC), skip it
      5. Calculate confidence: 1.0 if multiple sources agree, 0.7 if single source, 0.5 if ambiguous

    **extractors/sections.py:**
    - SECTION_PATTERNS: Dict of regex patterns for common CV sections (experience, education, skills, certifications, languages, publications, volunteer, projects, summary)
    - detect_sections(text: str) -> List[Tuple[str, int, int]]: Return (section_name, start_char, end_char)
    - get_section_text(text: str, sections: List, section_name: str) -> Optional[str]: Extract text for specific section

    Create __init__.py with exports.
  </action>
  <verify>
    Test contact extraction with sample text containing:
    - Email: "john.doe@example.com"
    - Phone: "+44 7911 123456"
    - LinkedIn: "https://linkedin.com/in/johndoe"
    - Name: "John Doe" (at start of text)

    Test section detection with text containing "Experience", "Education", "Skills" headings.
  </verify>
  <done>Contact extraction returns name, email, phone, URLs with confidence score. Section detection identifies CV sections.</done>
</task>

<task type="auto">
  <name>Task 3: Implement work history, education, and skills extraction</name>
  <files>python-src/extractors/work_history.py, python-src/extractors/education.py, python-src/extractors/skills.py, python-src/main.py</files>
  <action>
    **extractors/work_history.py:**
    - extract_work_history(text: str, nlp) -> List[WorkEntry]:
      1. Look for date ranges (MMM YYYY - MMM YYYY or similar patterns)
      2. Use spaCy NER for ORG entities (company names)
      3. Extract job titles (often in bold/larger font - use heuristics on text patterns)
      4. Group bullets/paragraphs following each entry as description/highlights
      5. Normalize dates with normalizers.dates
      6. Calculate confidence per entry based on completeness

    **extractors/education.py:**
    - extract_education(text: str, nlp) -> List[EducationEntry]:
      1. Use spaCy NER for ORG entities in education section
      2. Look for degree keywords (BSc, MSc, PhD, BA, MA, Bachelor, Master, Doctor)
      3. Extract field of study (text after degree keyword)
      4. Extract date ranges
      5. Look for grade patterns (First Class, 2:1, 3.8 GPA)
      6. Normalize dates, calculate confidence

    **extractors/skills.py:**
    - extract_skills(text: str) -> List[SkillGroup]:
      1. Preserve candidate's section headings (Technical Skills, Languages, etc.)
      2. Split by comma, bullet points, or newlines
      3. Group skills under their heading

    **Update main.py:**
    - Add 'extract_cv' action handler
    - Accepts { action: 'extract_cv', id: string, file_path: string }
    - Calls parse_document, then runs all extractors
    - Uses the preloaded nlp model (already in main.py)
    - Returns full ParsedCV structure with confidence scores
  </action>
  <verify>
    Test full extraction pipeline:
    - Create or use a sample CV with work history, education, skills
    - Run extract_cv action
    - Verify work_history has entries with company, position, dates
    - Verify education has entries with institution, degree
    - Verify skills groups are extracted
    - Verify all dates are in dd/mm/yyyy format
    - Verify confidence scores are present (0.0-1.0)
  </verify>
  <done>Full CV extraction returns ParsedCV with work history, education, skills, all normalized with confidence scores</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run `python -c "from schema.cv_schema import ParsedCV; print('ok')"` - schema imports
2. Run `python -c "from extractors.contact import extract_contacts; print('ok')"` - extractors import
3. Test date normalization: "3/2/2020" -> "03/02/2020" (British format)
4. Test contact extraction on sample text with email, phone, LinkedIn
5. Start main.py and test extract_cv action with a real CV file
6. Verify ParsedCV has all fields populated with confidence scores
</verification>

<success_criteria>
- Name extracted using spaCy PERSON entity, company names filtered out
- Email/phone/URL extracted using regex patterns
- Work history entries have company, position, dates, description
- Education entries have institution, degree, field, dates
- Skills preserve original groupings from CV
- All dates normalized to dd/mm/yyyy British format
- Confidence scores calculated (1.0 agreement, 0.7 single source, 0.5 ambiguous)
- extract_cv IPC action returns full ParsedCV structure
</success_criteria>

<output>
After completion, create `.planning/phases/02-parsing-pipeline/02-02-SUMMARY.md`
</output>
