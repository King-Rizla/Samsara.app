---
phase: 02.1-llm-extraction
plan: 02
type: execute
wave: 2
depends_on: ["02.1-01"]
files_modified:
  - python-src/extractors/work_history.py
  - python-src/extractors/education.py
  - python-src/extractors/skills.py
  - python-src/extractors/contact.py
  - python-src/main.py
  - python-src/schema/cv_schema.py
autonomous: false

must_haves:
  truths:
    - "Work history extraction uses LLM when available, falls back to regex"
    - "Education extraction uses LLM when available, falls back to regex"
    - "Skills extraction uses LLM when available, falls back to regex"
    - "Contact extraction uses regex first, LLM only if regex returns incomplete data (missing name, email, or phone)"
    - "Extraction completes in <5s when Ollama available"
    - "Extraction works without Ollama (graceful fallback)"
    - "Extraction metadata reports which method was used (llm vs regex)"
  artifacts:
    - path: "python-src/extractors/work_history.py"
      provides: "Hybrid LLM/regex work history extraction"
      contains: "extract_work_history_hybrid"
      min_lines: 80
    - path: "python-src/extractors/education.py"
      provides: "Hybrid LLM/regex education extraction"
      contains: "extract_education_hybrid"
      min_lines: 60
    - path: "python-src/extractors/skills.py"
      provides: "Hybrid LLM/regex skills extraction"
      contains: "extract_skills_hybrid"
      min_lines: 40
    - path: "python-src/schema/cv_schema.py"
      provides: "ParsedCV with extraction_methods field"
      contains: "extraction_methods"
      min_lines: 65
    - path: "python-src/main.py"
      provides: "LLM client initialization and hybrid extraction calls"
      contains: "OllamaClient"
      min_lines: 150
  key_links:
    - from: "python-src/main.py"
      to: "python-src/extractors/llm/client.py"
      via: "import and instantiate OllamaClient"
      pattern: "from extractors\\.llm import OllamaClient"
    - from: "python-src/extractors/work_history.py"
      to: "python-src/extractors/llm/client.py"
      via: "OllamaClient parameter in hybrid function"
      pattern: "llm_client.*OllamaClient"
    - from: "python-src/extractors/contact.py"
      to: "incomplete data detection"
      via: "check if name, email, or phone is missing before LLM fallback"
      pattern: "not contact\\.get\\('name'\\)|not contact\\.get\\('email'\\)|not contact\\.get\\('phone'\\)"
    - from: "python-src/main.py"
      to: "hybrid extraction functions"
      via: "calls to extract_work_history_hybrid etc"
      pattern: "extract_work_history_hybrid"
---

<objective>
Wire the LLM client into the existing extraction pipeline with hybrid fallback logic.

Purpose: Enhance CV extraction accuracy by using local LLM for semantic fields while maintaining reliability through regex fallback. The hybrid approach ensures the system works even when Ollama is unavailable, and tracks which extraction method was used for debugging/confidence scoring.

Output:
- Updated extractors with hybrid LLM/regex paths
- Updated main.py with LLM client initialization and hybrid extraction calls
- Updated cv_schema.py with extraction_methods field
- Extraction metadata reporting method used
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-llm-extraction/02.1-RESEARCH.md
@.planning/phases/02.1-llm-extraction/02.1-01-SUMMARY.md

# Existing code to modify
@python-src/main.py
@python-src/extractors/work_history.py
@python-src/extractors/education.py
@python-src/extractors/skills.py
@python-src/extractors/contact.py
@python-src/schema/cv_schema.py
@python-src/normalizers/dates.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add hybrid extraction functions to extractors</name>
  <files>
    python-src/extractors/work_history.py
    python-src/extractors/education.py
    python-src/extractors/skills.py
    python-src/extractors/contact.py
  </files>
  <action>
Add hybrid extraction functions that try LLM first, fall back to regex.

**1. Update `python-src/extractors/work_history.py`:**

Add imports at top of file:
```python
from typing import Tuple, Optional
from normalizers.dates import normalize_date
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMWorkHistory
from extractors.llm.prompts import WORK_HISTORY_PROMPT
```

Add `extract_work_history_hybrid()` function:

```python
def extract_work_history_hybrid(
    text: str,
    nlp,
    llm_client: Optional[OllamaClient] = None
) -> Tuple[List[WorkEntry], dict]:
    """
    Extract work history with LLM, fallback to regex.

    Returns:
        Tuple of (entries, metadata with method/llm_available)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try LLM extraction first
    if llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=WORK_HISTORY_PROMPT,
            schema=LLMWorkHistory,
            temperature=0.0
        )

        if llm_result and llm_result.entries:
            entries = []
            for e in llm_result.entries:
                entry = _llm_to_work_entry(e)
                if _validate_work_entry(entry):
                    entries.append(entry)

            if entries:
                metadata["method"] = "llm"
                return entries, metadata

    # Fallback to regex/NER
    entries = extract_work_history(text, nlp)
    return entries, metadata

def _llm_to_work_entry(llm_entry) -> WorkEntry:
    """Convert LLM output to schema WorkEntry."""
    return WorkEntry(
        company=llm_entry.company or '',
        position=llm_entry.position or '',
        start_date=normalize_date(llm_entry.start_date) if llm_entry.start_date else None,
        end_date=normalize_date(llm_entry.end_date) if llm_entry.end_date else None,
        description=llm_entry.description or '',
        highlights=llm_entry.highlights or [],
        confidence=0.85  # Higher confidence for LLM extraction
    )

def _validate_work_entry(entry: WorkEntry) -> bool:
    """Validate extracted work entry has minimum required data."""
    if not entry.get('company') and not entry.get('position'):
        return False
    return True
```

**2. Update `python-src/extractors/education.py`:**

Add imports at top of file:
```python
from typing import Tuple, Optional
from normalizers.dates import normalize_date
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMEducation
from extractors.llm.prompts import EDUCATION_PROMPT
```

Add `extract_education_hybrid()` function:

```python
def extract_education_hybrid(
    text: str,
    nlp,
    llm_client: Optional[OllamaClient] = None
) -> Tuple[List[EducationEntry], dict]:
    """
    Extract education with LLM, fallback to regex.

    Returns:
        Tuple of (entries, metadata with method/llm_available)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try LLM extraction first
    if llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=EDUCATION_PROMPT,
            schema=LLMEducation,
            temperature=0.0
        )

        if llm_result and llm_result.entries:
            entries = []
            for e in llm_result.entries:
                entry = _llm_to_education_entry(e)
                if _validate_education_entry(entry):
                    entries.append(entry)

            if entries:
                metadata["method"] = "llm"
                return entries, metadata

    # Fallback to regex/NER
    entries = extract_education(text, nlp)
    return entries, metadata

def _llm_to_education_entry(llm_entry) -> EducationEntry:
    """Convert LLM output to schema EducationEntry."""
    return EducationEntry(
        institution=llm_entry.institution or '',
        degree=llm_entry.degree or '',
        field_of_study=llm_entry.field_of_study,
        start_date=normalize_date(llm_entry.start_date) if llm_entry.start_date else None,
        end_date=normalize_date(llm_entry.end_date) if llm_entry.end_date else None,
        grade=llm_entry.grade,
        confidence=0.85  # Higher confidence for LLM extraction
    )

def _validate_education_entry(entry: EducationEntry) -> bool:
    """Validate extracted education entry has minimum required data."""
    if not entry.get('institution') and not entry.get('degree'):
        return False
    return True
```

**3. Update `python-src/extractors/skills.py`:**

Add imports at top of file:
```python
from typing import Tuple, Optional, List
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMSkills
from extractors.llm.prompts import SKILLS_PROMPT
from schema.cv_schema import SkillGroup
```

Add `extract_skills_hybrid()` function:

```python
def extract_skills_hybrid(
    text: str,
    llm_client: Optional[OllamaClient] = None
) -> Tuple[List[SkillGroup], dict]:
    """
    Extract skills with LLM, fallback to regex.

    Returns:
        Tuple of (skill_groups, metadata with method/llm_available)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try LLM extraction first
    if llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=SKILLS_PROMPT,
            schema=LLMSkills,
            temperature=0.0
        )

        if llm_result and llm_result.groups:
            skill_groups = []
            for g in llm_result.groups:
                group = _llm_to_skill_group(g)
                if group['skills']:  # Only add groups with actual skills
                    skill_groups.append(group)

            if skill_groups:
                metadata["method"] = "llm"
                return skill_groups, metadata

    # Fallback to regex
    skills = extract_skills(text)
    return skills, metadata

def _llm_to_skill_group(llm_group) -> SkillGroup:
    """Convert LLM output to schema SkillGroup."""
    return SkillGroup(
        category=llm_group.category or 'General',
        skills=llm_group.skills or []
    )
```

**4. Update `python-src/extractors/contact.py`:**

Add imports at top of file:
```python
from typing import Tuple, Optional
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMContact
from extractors.llm.prompts import CONTACT_PROMPT
```

Add `extract_contacts_hybrid()` function:

```python
def extract_contacts_hybrid(
    text: str,
    nlp,
    llm_client: Optional[OllamaClient] = None
) -> Tuple[ContactInfo, float, dict]:
    """
    Extract contact info with regex first, LLM as fallback for incomplete data.

    Contact info is deterministic - regex is reliable for emails, phones, URLs.
    Only use LLM if regex extraction is incomplete (missing name, email, or phone).

    Returns:
        Tuple of (contact, confidence, metadata with method/llm_available)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try regex extraction first (it's reliable for contact patterns)
    contact, confidence = extract_contacts(text, nlp)

    # Check if contact data is incomplete - missing name, email, or phone
    is_incomplete = (
        not contact.get('name') or
        not contact.get('email') or
        not contact.get('phone')
    )

    # Only try LLM if regex returned incomplete data AND LLM is available
    if is_incomplete and llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=CONTACT_PROMPT,
            schema=LLMContact,
            temperature=0.0
        )

        if llm_result:
            # Merge LLM results with regex results (regex takes priority for non-empty fields)
            merged = _merge_contact_info(contact, llm_result)
            if merged != contact:
                metadata["method"] = "hybrid"  # Both methods contributed
                return merged, 0.80, metadata

    return contact, confidence, metadata

def _merge_contact_info(regex_contact: ContactInfo, llm_contact) -> ContactInfo:
    """Merge LLM contact info with regex results. Regex takes priority for non-empty fields."""
    merged = ContactInfo(
        name=regex_contact.get('name') or llm_contact.name,
        email=regex_contact.get('email') or llm_contact.email,
        phone=regex_contact.get('phone') or llm_contact.phone,
        address=regex_contact.get('address') or llm_contact.address,
        linkedin=regex_contact.get('linkedin') or llm_contact.linkedin,
        github=regex_contact.get('github') or llm_contact.github,
        portfolio=regex_contact.get('portfolio') or llm_contact.portfolio,
    )
    return merged
```

Keep the existing extract_* functions - they are the fallback path.
  </action>
  <verify>
```bash
cd python-src
python -c "
from extractors.work_history import extract_work_history, extract_work_history_hybrid
from extractors.education import extract_education, extract_education_hybrid
from extractors.skills import extract_skills, extract_skills_hybrid
from extractors.contact import extract_contacts, extract_contacts_hybrid
print('All hybrid functions importable')
"
```
  </verify>
  <done>
- extract_work_history_hybrid() added with LLM-first, regex-fallback pattern
- extract_education_hybrid() added with same pattern
- extract_skills_hybrid() added with same pattern
- extract_contacts_hybrid() added with regex-first, LLM-fallback pattern (only when name, email, or phone missing)
- All functions include normalize_date import and conversion implementations
- All functions return metadata with method used (llm vs regex vs hybrid)
  </done>
</task>

<task type="auto">
  <name>Task 2: Update cv_schema.py and wire LLM client into main.py</name>
  <files>
    python-src/schema/cv_schema.py
    python-src/main.py
  </files>
  <action>
**1. Update `python-src/schema/cv_schema.py`:**

Add ExtractionMethods TypedDict and add it to ParsedCV:

```python
class ExtractionMethods(TypedDict, total=False):
    """Metadata about which extraction method was used for each field."""
    contact: str        # 'regex', 'llm', or 'hybrid'
    work_history: str   # 'regex' or 'llm'
    education: str      # 'regex' or 'llm'
    skills: str         # 'regex' or 'llm'
    llm_available: bool # Whether LLM was available during extraction


class ParsedCV(TypedDict, total=False):
    """Complete parsed CV structure."""
    contact: ContactInfo
    work_history: List[WorkEntry]
    education: List[EducationEntry]
    skills: List[SkillGroup]
    certifications: List[str]
    languages: List[str]
    other_sections: dict
    raw_text: str
    section_order: List[str]
    parse_confidence: float
    warnings: List[str]
    extraction_methods: ExtractionMethods  # NEW: Add this field
```

**2. Update main.py:**

Add imports:
```python
from extractors.llm import OllamaClient
from extractors.work_history import extract_work_history_hybrid
from extractors.education import extract_education_hybrid
from extractors.skills import extract_skills_hybrid
from extractors.contact import extract_contacts_hybrid
```

Initialize OllamaClient at startup (after spaCy load):
```python
# After nlp = spacy.load(...)
print(json.dumps({"status": "initializing_llm"}), flush=True)
llm_client = OllamaClient(model="qwen2.5:7b", timeout=60.0, keep_alive="5m")
llm_available = llm_client.is_available()
print(json.dumps({
    "status": "llm_initialized",
    "llm_available": llm_available,
    "model": "qwen2.5:7b" if llm_available else None
}), flush=True)
```

Update extract_cv action to use hybrid functions:

Replace:
```python
contact, contact_confidence = extract_contacts(raw_text, nlp)
work_history = extract_work_history(experience_text or raw_text, nlp)
education = extract_education(education_text or raw_text, nlp)
skills = extract_skills(skills_text or raw_text)
```

With:
```python
# Extract contact info (regex first, LLM fallback if incomplete)
contact, contact_confidence, contact_meta = extract_contacts_hybrid(raw_text, nlp, llm_client)

# Extract work history (LLM first, regex fallback)
work_history, work_meta = extract_work_history_hybrid(experience_text or raw_text, nlp, llm_client)

# Extract education (LLM first, regex fallback)
education, edu_meta = extract_education_hybrid(education_text or raw_text, nlp, llm_client)

# Extract skills (LLM first, regex fallback)
skills, skills_meta = extract_skills_hybrid(skills_text or raw_text, llm_client)
```

Add extraction_methods to response:
```python
parsed_cv: ParsedCV = {
    # ... existing fields ...
    'extraction_methods': {
        'contact': contact_meta.get('method', 'regex'),
        'work_history': work_meta.get('method', 'regex'),
        'education': edu_meta.get('method', 'regex'),
        'skills': skills_meta.get('method', 'regex'),
        'llm_available': llm_available
    }
}
```

Update health_check response to include llm_available field.
  </action>
  <verify>
```bash
cd python-src
# Test that main.py parses without errors
python -c "import main; print('main.py imports OK')"

# Test extract_cv action (requires a test CV file)
echo '{"action": "health_check", "id": "test"}' | python main.py
```
  </verify>
  <done>
- cv_schema.py updated with ExtractionMethods TypedDict and extraction_methods field in ParsedCV
- OllamaClient initialized at startup
- health_check reports llm_available status
- extract_cv uses hybrid extraction functions
- Response includes extraction_methods metadata showing which method used for each field
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete hybrid LLM extraction pipeline with fallback to regex/NER.

The extraction pipeline now:
1. Checks Ollama availability at startup
2. Uses LLM for work history, education, skills (when available)
3. Uses regex for contact info (LLM only as fallback if incomplete - missing name, email, or phone)
4. Falls back to regex for all fields when Ollama unavailable
5. Reports extraction method used in response metadata
  </what-built>
  <how-to-verify>
1. **Start the app:** `npm run start`
2. **Drop a test CV** into the drop zone
3. **Check console/response for:**
   - extraction_methods showing "llm" or "regex" for each field
   - If Ollama running: work_history/education/skills should show "llm"
   - Contact should show "regex" (unless incomplete contact data, then "hybrid")
4. **Verify extraction quality:**
   - Work history entries have company, position, dates, descriptions
   - Education entries have institution, degree, field, dates
   - Skills preserve candidate's groupings
5. **Test without Ollama:**
   - Stop Ollama service
   - Restart app and drop CV
   - Should still extract (method: "regex" for all)
   - No errors or crashes
6. **Check timing:** Extraction should complete in <5s with LLM, <2s without
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 2.1, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Hybrid extraction works:**
   - With Ollama: semantic fields use LLM (method: "llm")
   - Without Ollama: all fields use regex (method: "regex")

2. **Quality improvement:**
   - Work history extraction captures company, position, dates, description, highlights
   - Education extraction captures institution, degree, field, dates, grade
   - Skills preserve candidate's original groupings

3. **Performance:**
   - With LLM: <5s total extraction time
   - Without LLM: <2s total extraction time

4. **Graceful degradation:**
   - No crashes when Ollama unavailable
   - No exceptions propagate to response (always returns data)

Run full pipeline test:
```bash
cd python-src
python -c "
import json

# Simulate extract_cv request
request = {'action': 'extract_cv', 'file_path': 'test_cv.pdf', 'id': 'test'}
# Would need actual test file

# Test imports and client init
from extractors.llm import OllamaClient
from extractors.work_history import extract_work_history_hybrid
import spacy

nlp = spacy.load('en_core_web_sm', disable=['parser', 'lemmatizer'])
client = OllamaClient()
print(f'LLM available: {client.is_available()}')

# Test hybrid extraction
test_text = '''
Senior Software Engineer at Acme Corp
January 2020 - Present
- Led development of new features
- Managed team of 5 developers
'''
entries, meta = extract_work_history_hybrid(test_text, nlp, client)
print(f'Extraction method: {meta[\"method\"]}')
print(f'Entries found: {len(entries)}')
for e in entries:
    print(f'  - {e.get(\"position\")} at {e.get(\"company\")}')
"
```
</verification>

<success_criteria>
- [ ] extract_work_history_hybrid() tries LLM first, falls back to regex
- [ ] extract_education_hybrid() tries LLM first, falls back to regex
- [ ] extract_skills_hybrid() tries LLM first, falls back to regex
- [ ] extract_contacts_hybrid() tries regex first, LLM only when name/email/phone missing
- [ ] cv_schema.py has ExtractionMethods TypedDict and extraction_methods in ParsedCV
- [ ] main.py initializes OllamaClient at startup
- [ ] extract_cv action uses hybrid functions
- [ ] Response includes extraction_methods metadata
- [ ] health_check reports llm_available status
- [ ] Extraction works without Ollama (graceful fallback)
- [ ] Extraction completes in <5s with LLM
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-llm-extraction/02.1-02-SUMMARY.md`
</output>
