---
phase: 02.1-llm-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - python-src/extractors/llm/__init__.py
  - python-src/extractors/llm/client.py
  - python-src/extractors/llm/schemas.py
  - python-src/extractors/llm/prompts.py
  - python-src/requirements.txt
autonomous: true

user_setup:
  - service: ollama
    why: "Local LLM inference for CV extraction"
    env_vars: []
    dashboard_config:
      - task: "Install Ollama"
        location: "Download from ollama.ai and install"
      - task: "Pull Qwen 2.5 7B model"
        location: "Terminal: ollama pull qwen2.5:7b"

must_haves:
  truths:
    - "Ollama client detects whether Ollama is running and model is available"
    - "Pydantic schemas generate valid JSON schemas for Ollama structured output"
    - "Prompts guide LLM to extract work history, education, skills, and contact accurately"
    - "Client handles timeout and returns None on failure (for fallback)"
  artifacts:
    - path: "python-src/extractors/llm/__init__.py"
      provides: "Module exports"
      min_lines: 5
    - path: "python-src/extractors/llm/client.py"
      provides: "OllamaClient class with is_available() and extract() methods"
      contains: "class OllamaClient"
      min_lines: 50
    - path: "python-src/extractors/llm/schemas.py"
      provides: "Pydantic models for LLM output (LLMWorkHistory, LLMEducation, LLMSkills, LLMContact)"
      contains: "class LLMWorkHistory"
      min_lines: 60
    - path: "python-src/extractors/llm/prompts.py"
      provides: "Extraction prompts (WORK_HISTORY_PROMPT, EDUCATION_PROMPT, SKILLS_PROMPT, CONTACT_PROMPT)"
      contains: "WORK_HISTORY_PROMPT, EDUCATION_PROMPT, SKILLS_PROMPT, CONTACT_PROMPT"
      min_lines: 40
    - path: "python-src/requirements.txt"
      provides: "ollama dependency added"
      contains: "ollama"
  key_links:
    - from: "python-src/extractors/llm/client.py"
      to: "ollama.Client"
      via: "import and instantiation"
      pattern: "from ollama import Client"
    - from: "python-src/extractors/llm/client.py"
      to: "python-src/extractors/llm/schemas.py"
      via: "schema parameter in extract()"
      pattern: "schema\\.model_json_schema"
---

<objective>
Create the Ollama integration layer for LLM-based CV extraction.

Purpose: Enable local LLM (Ollama with Qwen 2.5 7B) to extract structured data from CVs with better accuracy than regex alone. This provides the foundation for hybrid extraction where LLM handles semantic fields (work history, education, skills) while regex handles deterministic patterns (contact info).

Output:
- `python-src/extractors/llm/` module with client, schemas, and prompts
- Updated requirements.txt with ollama dependency
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-llm-extraction/02.1-RESEARCH.md

# Existing extraction code (for schema compatibility)
@python-src/schema/cv_schema.py
@python-src/extractors/work_history.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM module with client, schemas, and prompts</name>
  <files>
    python-src/extractors/llm/__init__.py
    python-src/extractors/llm/client.py
    python-src/extractors/llm/schemas.py
    python-src/extractors/llm/prompts.py
  </files>
  <action>
Create the LLM extraction module under `python-src/extractors/llm/`.

**1. Create `python-src/extractors/llm/__init__.py`:**
Export OllamaClient and schema classes for easy importing.

**2. Create `python-src/extractors/llm/client.py`:**
Implement OllamaClient class following the pattern from RESEARCH.md:

```python
class OllamaClient:
    def __init__(self, model="qwen2.5:7b", timeout=60.0, keep_alive="5m"):
        # Store config, lazy init client

    def is_available(self) -> bool:
        # Check if Ollama running AND model pulled
        # Cache result to avoid repeated checks
        # Use 5s timeout for health check (not full 60s)

    def extract(self, text: str, prompt: str, schema: Type[T], temperature=0.0) -> Optional[T]:
        # Call Ollama chat with structured output
        # Use schema.model_json_schema() for format param
        # Return None on any failure (for fallback)
```

Key implementation details:
- Use `from ollama import Client` (not ollama.chat function)
- Set timeout via Client constructor: `Client(timeout=self.timeout)`
- Check model availability by parsing ollama.list() response
- Base model matching: "qwen2.5:7b" matches "qwen2.5:7b-instruct-q4_K_M" etc.
- Catch ALL exceptions and return None - never raise (fallback will handle)
- Use temperature=0 for deterministic extraction

**3. Create `python-src/extractors/llm/schemas.py`:**
Define Pydantic models that generate JSON schemas for Ollama:

- `LLMWorkEntry` with company, position, start_date, end_date, description, highlights
- `LLMWorkHistory` with entries: List[LLMWorkEntry]
- `LLMEducationEntry` with institution, degree, field_of_study, start_date, end_date, grade
- `LLMEducation` with entries: List[LLMEducationEntry]
- `LLMSkillGroup` with category, skills
- `LLMSkills` with groups: List[LLMSkillGroup]
- `LLMContact` with name, email, phone, address, linkedin, github, portfolio

Use Field() with descriptions - these become part of JSON schema and guide LLM.
Use Optional[] and default_factory=list appropriately.

**4. Create `python-src/extractors/llm/prompts.py`:**
Define extraction prompts from RESEARCH.md:

- `WORK_HISTORY_PROMPT` - Extract work entries, British date format, preserve wording
- `EDUCATION_PROMPT` - Extract education, recognize UK qualifications
- `SKILLS_PROMPT` - Preserve candidate's own groupings/categories
- `CONTACT_PROMPT` - Fallback for contact extraction if regex fails

Each prompt must end with "Return as JSON only. Do not include any explanation."
  </action>
  <verify>
Run Python import test:
```bash
cd python-src
python -c "from extractors.llm import OllamaClient; from extractors.llm.schemas import LLMWorkHistory, LLMEducation, LLMSkills; print('Imports OK')"
```
Verify schema generation:
```bash
python -c "from extractors.llm.schemas import LLMWorkHistory; import json; print(json.dumps(LLMWorkHistory.model_json_schema(), indent=2))"
```
  </verify>
  <done>
LLM module importable. OllamaClient class exists with is_available() and extract() methods. Pydantic schemas generate valid JSON schemas. Prompts defined for work history, education, skills, and contact extraction.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add ollama dependency and test client availability check</name>
  <files>
    python-src/requirements.txt
  </files>
  <action>
**1. Update `python-src/requirements.txt`:**
Add ollama dependency:
```
ollama>=0.4.0
```

**2. Install dependency:**
```bash
cd python-src
pip install -r requirements.txt
```

**3. Test client availability detection:**
Create a test script that:
- Creates OllamaClient instance
- Calls is_available() and reports result
- If available, tries a simple extract to verify model works
- If not available, confirms graceful False return (no exception)

Test both scenarios:
- When Ollama is running with model pulled: should return True
- When Ollama not running: should return False (not raise exception)
  </action>
  <verify>
```bash
cd python-src
python -c "
from extractors.llm import OllamaClient
client = OllamaClient()
available = client.is_available()
print(f'Ollama available: {available}')
if available:
    from extractors.llm.schemas import LLMSkills
    from extractors.llm.prompts import SKILLS_PROMPT
    result = client.extract('Python, JavaScript, SQL', SKILLS_PROMPT, LLMSkills)
    print(f'Extract test: {result}')
else:
    print('Ollama not running - fallback will be used')
"
```
Output should show availability status without crashing.
  </verify>
  <done>
- ollama added to requirements.txt
- Dependency installed successfully
- OllamaClient.is_available() returns True when Ollama running with model, False otherwise (no exceptions)
- If available, extract() returns parsed Pydantic model; if unavailable, returns None
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Import test**: All LLM module components importable
2. **Schema test**: Pydantic schemas generate valid JSON for Ollama format parameter
3. **Availability test**: Client gracefully detects Ollama presence
4. **No exceptions**: Missing Ollama returns False/None, never raises

Run comprehensive check:
```bash
cd python-src
python -c "
from extractors.llm import OllamaClient
from extractors.llm.schemas import LLMWorkHistory, LLMEducation, LLMSkills
from extractors.llm.prompts import WORK_HISTORY_PROMPT, EDUCATION_PROMPT, SKILLS_PROMPT, CONTACT_PROMPT

# Schema check
import json
for schema in [LLMWorkHistory, LLMEducation, LLMSkills]:
    js = schema.model_json_schema()
    assert 'properties' in js, f'{schema.__name__} missing properties'
    print(f'{schema.__name__}: OK')

# Client check
client = OllamaClient()
print(f'Ollama available: {client.is_available()}')
print('All checks passed')
"
```
</verification>

<success_criteria>
- [ ] `python-src/extractors/llm/` directory exists with __init__.py, client.py, schemas.py, prompts.py
- [ ] OllamaClient class with is_available() that returns bool (never raises)
- [ ] OllamaClient.extract() returns Pydantic model or None (never raises)
- [ ] LLMWorkHistory, LLMEducation, LLMSkills Pydantic models generate JSON schemas
- [ ] Prompts defined for work history, education, skills, contact
- [ ] ollama>=0.4.0 in requirements.txt and installed
- [ ] Tests pass regardless of whether Ollama is running
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-llm-extraction/02.1-01-SUMMARY.md`
</output>
