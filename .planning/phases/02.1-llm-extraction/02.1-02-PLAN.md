---
phase: 02.1-llm-extraction
plan: 02
type: execute
wave: 2
depends_on: ["02.1-01"]
files_modified:
  - python-src/extractors/work_history.py
  - python-src/extractors/education.py
  - python-src/extractors/skills.py
  - python-src/extractors/contact.py
  - python-src/main.py
autonomous: false

must_haves:
  truths:
    - "Work history extraction uses LLM when available, falls back to regex"
    - "Education extraction uses LLM when available, falls back to regex"
    - "Skills extraction uses LLM when available, falls back to regex"
    - "Contact extraction uses regex first, LLM only if regex returns incomplete data"
    - "Extraction completes in <5s when Ollama available"
    - "Extraction works without Ollama (graceful fallback)"
    - "Extraction metadata reports which method was used (llm vs regex)"
  artifacts:
    - path: "python-src/extractors/work_history.py"
      provides: "Hybrid LLM/regex work history extraction"
      contains: "extract_work_history_hybrid"
      min_lines: 80
    - path: "python-src/extractors/education.py"
      provides: "Hybrid LLM/regex education extraction"
      contains: "extract_education_hybrid"
      min_lines: 60
    - path: "python-src/extractors/skills.py"
      provides: "Hybrid LLM/regex skills extraction"
      contains: "extract_skills_hybrid"
      min_lines: 40
    - path: "python-src/main.py"
      provides: "LLM client initialization and hybrid extraction calls"
      contains: "OllamaClient"
      min_lines: 150
  key_links:
    - from: "python-src/main.py"
      to: "python-src/extractors/llm/client.py"
      via: "import and instantiate OllamaClient"
      pattern: "from extractors\\.llm import OllamaClient"
    - from: "python-src/extractors/work_history.py"
      to: "python-src/extractors/llm/client.py"
      via: "OllamaClient parameter in hybrid function"
      pattern: "llm_client.*OllamaClient"
    - from: "python-src/main.py"
      to: "hybrid extraction functions"
      via: "calls to extract_work_history_hybrid etc"
      pattern: "extract_work_history_hybrid"
---

<objective>
Wire the LLM client into the existing extraction pipeline with hybrid fallback logic.

Purpose: Enhance CV extraction accuracy by using local LLM for semantic fields while maintaining reliability through regex fallback. The hybrid approach ensures the system works even when Ollama is unavailable, and tracks which extraction method was used for debugging/confidence scoring.

Output:
- Updated extractors with hybrid LLM/regex paths
- Updated main.py with LLM client initialization and hybrid extraction calls
- Extraction metadata reporting method used
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-llm-extraction/02.1-RESEARCH.md
@.planning/phases/02.1-llm-extraction/02.1-01-SUMMARY.md

# Existing code to modify
@python-src/main.py
@python-src/extractors/work_history.py
@python-src/extractors/education.py
@python-src/extractors/skills.py
@python-src/extractors/contact.py
@python-src/schema/cv_schema.py
@python-src/normalizers/dates.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add hybrid extraction functions to extractors</name>
  <files>
    python-src/extractors/work_history.py
    python-src/extractors/education.py
    python-src/extractors/skills.py
    python-src/extractors/contact.py
  </files>
  <action>
Add hybrid extraction functions that try LLM first, fall back to regex.

**1. Update `python-src/extractors/work_history.py`:**

Add `extract_work_history_hybrid()` function:

```python
from typing import Tuple, Optional
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMWorkHistory
from extractors.llm.prompts import WORK_HISTORY_PROMPT

def extract_work_history_hybrid(
    text: str,
    nlp,
    llm_client: Optional[OllamaClient] = None
) -> Tuple[List[WorkEntry], dict]:
    """
    Extract work history with LLM, fallback to regex.

    Returns:
        Tuple of (entries, metadata with method/llm_available)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try LLM extraction first
    if llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=WORK_HISTORY_PROMPT,
            schema=LLMWorkHistory,
            temperature=0.0
        )

        if llm_result and llm_result.entries:
            entries = []
            for e in llm_result.entries:
                entry = _llm_to_work_entry(e)
                if _validate_work_entry(entry):
                    entries.append(entry)

            if entries:
                metadata["method"] = "llm"
                return entries, metadata

    # Fallback to regex/NER
    entries = extract_work_history(text, nlp)
    return entries, metadata

def _llm_to_work_entry(llm_entry) -> WorkEntry:
    """Convert LLM output to schema WorkEntry."""
    return WorkEntry(
        company=llm_entry.company or '',
        position=llm_entry.position or '',
        start_date=normalize_date(llm_entry.start_date) if llm_entry.start_date else None,
        end_date=normalize_date(llm_entry.end_date) if llm_entry.end_date else None,
        description=llm_entry.description or '',
        highlights=llm_entry.highlights or [],
        confidence=0.85  # Higher confidence for LLM extraction
    )

def _validate_work_entry(entry: WorkEntry) -> bool:
    """Validate extracted work entry has minimum required data."""
    if not entry.get('company') and not entry.get('position'):
        return False
    return True
```

**2. Update `python-src/extractors/education.py`:**

Add `extract_education_hybrid()` following same pattern:
- Try LLM first with LLMEducation schema and EDUCATION_PROMPT
- Convert LLMEducationEntry to EducationEntry via _llm_to_education_entry()
- Fall back to existing extract_education() on failure
- Return (entries, metadata)

**3. Update `python-src/extractors/skills.py`:**

Add `extract_skills_hybrid()`:
- Try LLM first with LLMSkills schema and SKILLS_PROMPT
- Convert LLMSkillGroup to SkillGroup
- Fall back to existing extract_skills() on failure
- Return (skills, metadata)

**4. Update `python-src/extractors/contact.py`:**

Add `extract_contacts_hybrid()`:
- Different pattern: regex FIRST (contact is deterministic)
- Only try LLM if regex returned incomplete data (missing name, email, or phone)
- Use CONTACT_PROMPT for LLM fallback
- Return (contact, confidence, metadata)

Keep the existing extract_* functions - they are the fallback path.
  </action>
  <verify>
```bash
cd python-src
python -c "
from extractors.work_history import extract_work_history, extract_work_history_hybrid
from extractors.education import extract_education, extract_education_hybrid
from extractors.skills import extract_skills, extract_skills_hybrid
from extractors.contact import extract_contacts, extract_contacts_hybrid
print('All hybrid functions importable')
"
```
  </verify>
  <done>
- extract_work_history_hybrid() added with LLM-first, regex-fallback pattern
- extract_education_hybrid() added with same pattern
- extract_skills_hybrid() added with same pattern
- extract_contacts_hybrid() added with regex-first, LLM-fallback pattern
- All functions return metadata with method used (llm vs regex)
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire LLM client into main.py extraction flow</name>
  <files>
    python-src/main.py
  </files>
  <action>
Update main.py to initialize OllamaClient and use hybrid extraction functions.

**1. Add imports:**
```python
from extractors.llm import OllamaClient
from extractors.work_history import extract_work_history_hybrid
from extractors.education import extract_education_hybrid
from extractors.skills import extract_skills_hybrid
from extractors.contact import extract_contacts_hybrid
```

**2. Initialize OllamaClient at startup (after spaCy load):**
```python
# After nlp = spacy.load(...)
print(json.dumps({"status": "initializing_llm"}), flush=True)
llm_client = OllamaClient(model="qwen2.5:7b", timeout=60.0, keep_alive="5m")
llm_available = llm_client.is_available()
print(json.dumps({
    "status": "llm_initialized",
    "llm_available": llm_available,
    "model": "qwen2.5:7b" if llm_available else None
}), flush=True)
```

**3. Update extract_cv action to use hybrid functions:**

Replace:
```python
contact, contact_confidence = extract_contacts(raw_text, nlp)
work_history = extract_work_history(experience_text or raw_text, nlp)
education = extract_education(education_text or raw_text, nlp)
skills = extract_skills(skills_text or raw_text)
```

With:
```python
# Extract contact info (regex first, LLM fallback)
contact, contact_confidence, contact_meta = extract_contacts_hybrid(raw_text, nlp, llm_client)

# Extract work history (LLM first, regex fallback)
work_history, work_meta = extract_work_history_hybrid(experience_text or raw_text, nlp, llm_client)

# Extract education (LLM first, regex fallback)
education, edu_meta = extract_education_hybrid(education_text or raw_text, nlp, llm_client)

# Extract skills (LLM first, regex fallback)
skills, skills_meta = extract_skills_hybrid(skills_text or raw_text, llm_client)
```

**4. Add extraction_methods to response:**
```python
parsed_cv: ParsedCV = {
    # ... existing fields ...
    'extraction_methods': {
        'contact': contact_meta.get('method', 'regex'),
        'work_history': work_meta.get('method', 'regex'),
        'education': edu_meta.get('method', 'regex'),
        'skills': skills_meta.get('method', 'regex'),
        'llm_available': llm_available
    }
}
```

**5. Update health_check response:**
Add llm_available field to health check response.
  </action>
  <verify>
```bash
cd python-src
# Test that main.py parses without errors
python -c "import main; print('main.py imports OK')"

# Test extract_cv action (requires a test CV file)
echo '{"action": "health_check", "id": "test"}' | python main.py
```
  </verify>
  <done>
- OllamaClient initialized at startup
- health_check reports llm_available status
- extract_cv uses hybrid extraction functions
- Response includes extraction_methods metadata showing which method used for each field
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete hybrid LLM extraction pipeline with fallback to regex/NER.

The extraction pipeline now:
1. Checks Ollama availability at startup
2. Uses LLM for work history, education, skills (when available)
3. Uses regex for contact info (LLM only as fallback if incomplete)
4. Falls back to regex for all fields when Ollama unavailable
5. Reports extraction method used in response metadata
  </what-built>
  <how-to-verify>
1. **Start the app:** `npm run start`
2. **Drop a test CV** into the drop zone
3. **Check console/response for:**
   - extraction_methods showing "llm" or "regex" for each field
   - If Ollama running: work_history/education/skills should show "llm"
   - Contact should show "regex" (unless incomplete contact data)
4. **Verify extraction quality:**
   - Work history entries have company, position, dates, descriptions
   - Education entries have institution, degree, field, dates
   - Skills preserve candidate's groupings
5. **Test without Ollama:**
   - Stop Ollama service
   - Restart app and drop CV
   - Should still extract (method: "regex" for all)
   - No errors or crashes
6. **Check timing:** Extraction should complete in <5s with LLM, <2s without
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 2.1, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Hybrid extraction works:**
   - With Ollama: semantic fields use LLM (method: "llm")
   - Without Ollama: all fields use regex (method: "regex")

2. **Quality improvement:**
   - Work history extraction captures company, position, dates, description, highlights
   - Education extraction captures institution, degree, field, dates, grade
   - Skills preserve candidate's original groupings

3. **Performance:**
   - With LLM: <5s total extraction time
   - Without LLM: <2s total extraction time

4. **Graceful degradation:**
   - No crashes when Ollama unavailable
   - No exceptions propagate to response (always returns data)

Run full pipeline test:
```bash
cd python-src
python -c "
import json

# Simulate extract_cv request
request = {'action': 'extract_cv', 'file_path': 'test_cv.pdf', 'id': 'test'}
# Would need actual test file

# Test imports and client init
from extractors.llm import OllamaClient
from extractors.work_history import extract_work_history_hybrid
import spacy

nlp = spacy.load('en_core_web_sm', disable=['parser', 'lemmatizer'])
client = OllamaClient()
print(f'LLM available: {client.is_available()}')

# Test hybrid extraction
test_text = '''
Senior Software Engineer at Acme Corp
January 2020 - Present
- Led development of new features
- Managed team of 5 developers
'''
entries, meta = extract_work_history_hybrid(test_text, nlp, client)
print(f'Extraction method: {meta[\"method\"]}')
print(f'Entries found: {len(entries)}')
for e in entries:
    print(f'  - {e.get(\"position\")} at {e.get(\"company\")}')
"
```
</verification>

<success_criteria>
- [ ] extract_work_history_hybrid() tries LLM first, falls back to regex
- [ ] extract_education_hybrid() tries LLM first, falls back to regex
- [ ] extract_skills_hybrid() tries LLM first, falls back to regex
- [ ] extract_contacts_hybrid() tries regex first, LLM as fallback
- [ ] main.py initializes OllamaClient at startup
- [ ] extract_cv action uses hybrid functions
- [ ] Response includes extraction_methods metadata
- [ ] health_check reports llm_available status
- [ ] Extraction works without Ollama (graceful fallback)
- [ ] Extraction completes in <5s with LLM
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-llm-extraction/02.1-02-SUMMARY.md`
</output>
