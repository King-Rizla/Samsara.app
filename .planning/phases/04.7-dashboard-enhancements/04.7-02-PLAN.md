---
phase: 04.7-dashboard-enhancements
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - python-src/extractors/llm/client.py
  - python-src/extractors/llm/openai_client.py
  - python-src/main.py
autonomous: true

must_haves:
  truths:
    - "Ollama client captures prompt_eval_count and eval_count from response"
    - "OpenAI client captures usage.prompt_tokens and usage.completion_tokens from response"
    - "Python extraction response includes token_usage object with model name"
  artifacts:
    - path: "python-src/main.py"
      provides: "Token usage in extraction response"
      contains: "token_usage"
    - path: "python-src/extractors/llm/client.py"
      provides: "Ollama token capture"
      contains: "prompt_eval_count"
  key_links:
    - from: "client.py"
      to: "main.py"
      via: "Return value with token_usage"
      pattern: "token_usage"
---

<objective>
Modify Python LLM clients to capture and return token usage from extraction responses.

Purpose: Enables tracking of actual token consumption for usage limits and cost visibility.
Output: Python responses include token_usage field with prompt_tokens, completion_tokens, total_tokens, and model name.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.7-dashboard-enhancements/04.7-RESEARCH.md
@.planning/phases/04.7-dashboard-enhancements/04.7-CONTEXT.md
@python-src/extractors/llm/client.py
@python-src/extractors/llm/openai_client.py
@python-src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Capture token counts in Ollama client</name>
  <files>python-src/extractors/llm/client.py</files>
  <action>
    Modify the OllamaClient class to capture and return token usage.

    1. Add instance variable in __init__:
       ```python
       self._last_token_usage: dict | None = None
       self._model_name: str = model  # Store model name
       ```

    2. After each self._client.chat() call that gets a response, capture token counts:
       ```python
       self._last_token_usage = {
           'prompt_tokens': response.prompt_eval_count or 0,
           'completion_tokens': response.eval_count or 0,
           'total_tokens': (response.prompt_eval_count or 0) + (response.eval_count or 0),
           'model': self._model_name,
       }
       ```

    3. Add a getter method:
       ```python
       def get_last_token_usage(self) -> dict:
           """Return token usage from the last extraction call."""
           return self._last_token_usage or {
               'prompt_tokens': 0,
               'completion_tokens': 0,
               'total_tokens': 0,
               'model': self._model_name,
           }
       ```

    4. If extraction fails/returns None, still set _last_token_usage with zeros so cost tracking remains consistent.

    NOTE: Ollama only provides token counts in non-streaming mode, which is already used for extraction (Research pitfall #2). Verify the response object has these fields by checking existing code.
  </action>
  <verify>python -m py_compile python-src/extractors/llm/client.py shows no syntax errors</verify>
  <done>OllamaClient stores and exposes token usage from last extraction</done>
</task>

<task type="auto">
  <name>Task 2: Capture token counts in OpenAI client</name>
  <files>python-src/extractors/llm/openai_client.py</files>
  <action>
    Modify the OpenAIClient class to capture and return token usage.

    1. Add instance variable in __init__:
       ```python
       self._last_token_usage: dict | None = None
       self._model_name: str = model  # Store model name (e.g., 'gpt-4o-mini')
       ```

    2. After each API call (likely self._client.beta.chat.completions.parse or similar), capture token counts:
       ```python
       self._last_token_usage = {
           'prompt_tokens': response.usage.prompt_tokens,
           'completion_tokens': response.usage.completion_tokens,
           'total_tokens': response.usage.total_tokens,
           'model': self._model_name,
       }
       ```

    3. Add the same getter method:
       ```python
       def get_last_token_usage(self) -> dict:
           """Return token usage from the last extraction call."""
           return self._last_token_usage or {
               'prompt_tokens': 0,
               'completion_tokens': 0,
               'total_tokens': 0,
               'model': self._model_name,
           }
       ```

    4. Initialize self._last_token_usage = None in __init__.

    Note: If using beta.chat.completions.parse, the response still has a .usage attribute - verify the exact attribute path in existing code.
  </action>
  <verify>python -m py_compile python-src/extractors/llm/openai_client.py shows no syntax errors</verify>
  <done>OpenAIClient stores and exposes token usage from last extraction</done>
</task>

<task type="auto">
  <name>Task 3: Include token usage in extraction response</name>
  <files>python-src/main.py</files>
  <action>
    Modify the extraction response in main.py to include token_usage.

    Find the code that handles CV extraction (the function/handler that:
    1. Gets the LLM client (OllamaClient or OpenAIClient based on mode)
    2. Calls extraction
    3. Returns JSON response via print()

    After successful extraction, before returning the response:

    1. Get token usage from the client:
       ```python
       token_usage = client.get_last_token_usage()
       ```

    2. Include in the response data:
       ```python
       result = {
           'id': request_id,
           'success': True,
           'data': {
               **parsed_cv_dict,  # existing CV data
               'token_usage': token_usage,  # ADD THIS
           }
       }
       ```

    3. Also apply the same pattern to JD extraction if it uses LLM.

    4. For failed extractions, include token_usage with zeros:
       ```python
       result = {
           'id': request_id,
           'success': False,
           'error': str(e),
           'data': {
               'token_usage': client.get_last_token_usage() if client else {
                   'prompt_tokens': 0,
                   'completion_tokens': 0,
                   'total_tokens': 0,
                   'model': None,
               }
           }
       }
       ```
       This ensures usage is tracked even for failed attempts (they still consume tokens).
  </action>
  <verify>python -m py_compile python-src/main.py shows no syntax errors</verify>
  <done>Extraction responses include token_usage field with model name</done>
</task>

</tasks>

<verification>
After all tasks:
1. Run `python -m py_compile python-src/main.py` - no errors
2. Run `python -m py_compile python-src/extractors/llm/client.py` - no errors
3. Run `python -m py_compile python-src/extractors/llm/openai_client.py` - no errors
4. If tests exist: `cd python-src && python -m pytest` passes
</verification>

<success_criteria>
- Both LLM clients have get_last_token_usage() method
- Extraction responses include token_usage object with: prompt_tokens, completion_tokens, total_tokens, model
- Token counts are captured correctly (non-zero for actual extractions)
- Model name included for future multi-model/provider support (per CONTEXT.md specifics)
- No Python syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/04.7-dashboard-enhancements/04.7-02-SUMMARY.md`
</output>
