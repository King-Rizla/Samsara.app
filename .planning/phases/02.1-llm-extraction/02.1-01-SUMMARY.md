---
phase: 02.1-llm-extraction
plan: 01
subsystem: extraction
tags: [ollama, pydantic, llm, qwen, structured-output]

# Dependency graph
requires:
  - phase: 02-parsing
    provides: cv_schema.py with WorkEntry, EducationEntry, SkillGroup TypedDicts
provides:
  - OllamaClient with is_available() and extract() methods
  - Pydantic schemas for LLM structured output (LLMWorkHistory, LLMEducation, LLMSkills, LLMContact)
  - Extraction prompts for work history, education, skills, and contact
affects: [02.1-02-hybrid-integration, 02.1-llm-extraction]

# Tech tracking
tech-stack:
  added: [ollama>=0.4.0]
  patterns: [graceful-fallback-extraction, pydantic-json-schema, cached-availability-check]

key-files:
  created:
    - python-src/extractors/llm/__init__.py
    - python-src/extractors/llm/client.py
    - python-src/extractors/llm/schemas.py
    - python-src/extractors/llm/prompts.py
  modified:
    - python-src/requirements.txt

key-decisions:
  - "Qwen 2.5 7B as default model for structured output (not Qwen3 which breaks JSON with thinking tags)"
  - "60s timeout for extraction requests to handle cold-start model loading"
  - "5-minute keep_alive to avoid repeated model loading"
  - "Never raise exceptions - always return None for fallback handling"

patterns-established:
  - "Cached availability check: is_available() caches result to avoid repeated checks"
  - "Graceful degradation: extract() returns None on any failure, never raises"
  - "Lazy client init: Client only created if Ollama available"

# Metrics
duration: 5min
completed: 2026-01-25
---

# Phase 2.1 Plan 01: Ollama Integration Layer Summary

**OllamaClient with Pydantic schemas for structured LLM extraction, graceful fallback when unavailable**

## Performance

- **Duration:** 5 min
- **Started:** 2026-01-25T11:35:23Z
- **Completed:** 2026-01-25T11:40:19Z
- **Tasks:** 2
- **Files modified:** 5

## Accomplishments
- OllamaClient class with cached availability detection and structured extraction
- Pydantic schemas generating JSON schemas for Ollama format parameter
- Extraction prompts for work history, education, skills, and contact
- Graceful fallback pattern: never raises, returns None for regex fallback

## Task Commits

Each task was committed atomically:

1. **Task 1: Create LLM module with client, schemas, and prompts** - `503e830` (feat)
2. **Task 2: Add ollama dependency and test client availability check** - `da7a377` (chore)

## Files Created/Modified
- `python-src/extractors/llm/__init__.py` - Module exports for OllamaClient and schemas
- `python-src/extractors/llm/client.py` - OllamaClient with is_available() and extract()
- `python-src/extractors/llm/schemas.py` - Pydantic models: LLMWorkHistory, LLMEducation, LLMSkills, LLMContact
- `python-src/extractors/llm/prompts.py` - Extraction prompts with British date format guidance
- `python-src/requirements.txt` - Added ollama>=0.4.0

## Decisions Made
- **Qwen 2.5 7B default:** Qwen3 breaks JSON parsing with `<think>` tags; Qwen 2.5 works correctly
- **60s timeout:** 7B models need 60s minimum for cold-start requests
- **5m keep_alive:** Keeps model loaded to avoid cold-start on subsequent requests
- **Never raise exceptions:** All failures return None to enable regex fallback

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered
- **Cold-start timeout on first extraction:** Initial extract() call returned None due to model loading time. Subsequent calls (with warmed model) work correctly. This is expected behavior documented in RESEARCH.md.

## User Setup Required

**External service requires installation.** User must:
1. Install Ollama from ollama.ai
2. Pull Qwen 2.5 7B model: `ollama pull qwen2.5:7b`

The client gracefully returns `is_available() = False` if Ollama is not installed.

## Next Phase Readiness
- OllamaClient ready for hybrid integration (Plan 02)
- Schemas compatible with existing cv_schema.py TypedDicts
- Prompts tested and generating valid JSON responses

---
*Phase: 02.1-llm-extraction*
*Completed: 2026-01-25*
