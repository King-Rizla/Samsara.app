# Phase 4.7 Plan 02: Python Token Capture - Summary

**One-liner:** Added token usage tracking to OllamaClient and OpenAIClient, exposing prompt_tokens/completion_tokens/total_tokens/model in all extraction responses.

## What Was Built

### Token Capture in LLM Clients

Both `OllamaClient` and `OpenAIClient` now capture and expose token usage from their extraction calls:

**OllamaClient (`python-src/extractors/llm/client.py`):**
- Added `_last_token_usage` and `_model_name` instance variables
- Captures `prompt_eval_count` and `eval_count` from Ollama response
- `get_last_token_usage()` method returns token data with model name
- Sets zeros on extraction failure for consistent cost tracking

**OpenAIClient (`python-src/extractors/llm/openai_client.py`):**
- Added `_last_token_usage` and `_model_name` instance variables
- Captures `usage.prompt_tokens` and `usage.completion_tokens` from response
- `get_last_token_usage()` method returns token data with model name
- Sets zeros on extraction failure for consistent cost tracking

### Token Usage in Responses

**CV Extraction (`extract_cv` action):**
- Success response now includes `token_usage` field in data
- All error responses (FileNotFoundError, ValueError, Exception) include `token_usage` in data
- Enables tracking even for failed extractions (they still consume tokens)

**JD Extraction (`extract_jd` action):**
- Success response now includes `token_usage` field in data
- "LLM returned no result" error includes `token_usage`
- Generic exception handler includes `token_usage`

### Token Usage Schema

```json
{
  "token_usage": {
    "prompt_tokens": 1234,
    "completion_tokens": 567,
    "total_tokens": 1801,
    "model": "qwen2.5:7b"  // or "gpt-4o-mini"
  }
}
```

## Verification Results

| Check | Result |
|-------|--------|
| `py_compile client.py` | Pass |
| `py_compile openai_client.py` | Pass |
| `py_compile main.py` | Pass |

## Commits

| Hash | Message |
|------|---------|
| a769e60 | feat(04.7-02): capture token usage in OllamaClient |
| 0e56ffc | feat(04.7-02): capture token usage in OpenAIClient |
| 4a16019 | feat(04.7-02): include token_usage in extraction responses |

## Files Modified

- `python-src/extractors/llm/client.py` - OllamaClient token tracking
- `python-src/extractors/llm/openai_client.py` - OpenAIClient token tracking
- `python-src/main.py` - Token usage in extraction responses

## Deviations from Plan

None - plan executed exactly as written.

## Next Phase Readiness

**Enables:**
- Plan 04.7-03: TypeScript can now receive `token_usage` in extraction responses
- Plan 04.7-04: Usage can be aggregated and displayed in UI
- Plan 04.7-05: Limits can check against actual token consumption

**Dependencies satisfied:**
- Both LLM clients have `get_last_token_usage()` method
- Extraction responses include `token_usage` with model name
- Token counts tracked even for failed extractions

---

*Plan: 04.7-02*
*Completed: 2026-01-27*
*Duration: ~4 minutes*
