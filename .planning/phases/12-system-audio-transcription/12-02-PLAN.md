---
phase: 12-system-audio-transcription
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - python-src/audio/transcriber.py
  - python-src/main.py
  - src/main/transcriptionService.ts
  - src/main/audioRecordingService.ts
  - src/main/index.ts
  - src/main/preload.ts
  - src/renderer/stores/recordingStore.ts
  - src/renderer/components/recording/RecordingPanel.tsx
  - src/renderer/components/recording/WaveformMeter.tsx
  - src/renderer/components/recording/CandidateSelect.tsx
  - src/renderer/components/wheel/ProjectLayout.tsx
  - src/renderer/components/outreach/CallRecordCard.tsx
  - src/renderer/components/outreach/CandidatePanel.tsx
autonomous: true

must_haves:
  truths:
    - "User sees floating recording panel accessible from any wheel section"
    - "User can start/stop recording with visual level meter feedback"
    - "User can select a graduated candidate and attach recording to them"
    - "Recording is transcribed locally via faster-whisper after attachment"
    - "Transcript appears in candidate panel with 'Recruiter Call' badge"
  artifacts:
    - path: "python-src/audio/transcriber.py"
      provides: "LocalTranscriber with faster-whisper integration"
      min_lines: 60
    - path: "src/renderer/components/recording/RecordingPanel.tsx"
      provides: "Floating draggable recording control panel"
      min_lines: 100
    - path: "src/renderer/stores/recordingStore.ts"
      provides: "Recording state management with Zustand"
      exports: ["useRecordingStore"]
    - path: "src/main/transcriptionService.ts"
      provides: "Transcription job queue and status tracking"
      exports: ["queueTranscription", "getTranscriptionStatus"]
  key_links:
    - from: "src/renderer/components/recording/RecordingPanel.tsx"
      to: "window.api.startRecording"
      via: "IPC call on button click"
      pattern: "api\\.startRecording"
    - from: "src/main/transcriptionService.ts"
      to: "python-src/main.py"
      via: "sendToPython transcribe_audio action"
      pattern: "sendToPython.*transcribe_audio"
    - from: "src/renderer/components/outreach/CandidatePanel.tsx"
      to: "callRecords"
      via: "loadCallRecordsForCandidate includes recruiter calls"
      pattern: "type.*recruiter"
---

<objective>
Create floating recording panel UI, faster-whisper transcription in Python sidecar, and integrate recruiter call transcripts into candidate panel.

Purpose: Complete the recording workflow so recruiters can record calls, attach them to candidates, get local transcriptions, and view transcripts alongside AI screening calls.

Output: RecordingPanel floating component, transcriber module in Python, transcription job queue, "Recruiter Call" badge styling in CallRecordCard.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-system-audio-transcription/12-RESEARCH.md
@.planning/phases/12-system-audio-transcription/12-CONTEXT.md
@.planning/phases/12-system-audio-transcription/12-01-SUMMARY.md
@src/main/audioRecordingService.ts
@src/renderer/components/outreach/CallRecordCard.tsx
@src/renderer/components/outreach/CandidatePanel.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create faster-whisper transcriber and add transcribe_audio action</name>
  <files>
    python-src/audio/transcriber.py
    python-src/audio/__init__.py
    python-src/main.py
  </files>
  <action>
**python-src/audio/transcriber.py:**
Implement LocalTranscriber class following RESEARCH.md pattern:

```python
from faster_whisper import WhisperModel
from typing import Optional, Dict, Any
import os

class LocalTranscriber:
    """Local speech-to-text using faster-whisper. Runs on CPU."""

    def __init__(self, model_size: str = "small"):
        self.model_size = model_size
        self.model: Optional[WhisperModel] = None
        self._loaded = False

    def _ensure_loaded(self):
        """Lazy-load model on first use."""
        if self._loaded:
            return

        print(f"[Transcriber] Loading faster-whisper {self.model_size}...")

        # CPU with INT8 quantization for efficiency
        self.model = WhisperModel(
            self.model_size,
            device="cpu",
            compute_type="int8",
            download_root=self._get_model_dir()
        )

        self._loaded = True
        print(f"[Transcriber] Model loaded")

    def _get_model_dir(self) -> str:
        """Get model cache directory."""
        cache_dir = os.environ.get('SAMSARA_MODEL_DIR')
        if cache_dir:
            return cache_dir
        return os.path.join(os.path.expanduser('~'), '.samsara', 'models')

    def transcribe(self, audio_path: str, language: str = "en") -> Dict[str, Any]:
        """Transcribe audio file. Returns text, segments, language, duration."""
        self._ensure_loaded()

        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")

        segments, info = self.model.transcribe(
            audio_path,
            beam_size=5,
            language=language,
            vad_filter=True,
            vad_parameters={
                "min_silence_duration_ms": 500,
                "speech_pad_ms": 200
            },
            word_timestamps=False,
            condition_on_previous_text=True
        )

        segment_list = []
        full_text_parts = []

        for segment in segments:
            segment_list.append({
                "start": round(segment.start, 2),
                "end": round(segment.end, 2),
                "text": segment.text.strip()
            })
            full_text_parts.append(segment.text.strip())

        return {
            "text": " ".join(full_text_parts),
            "segments": segment_list,
            "language": info.language,
            "duration": round(info.duration, 2)
        }

    def is_available(self) -> bool:
        """Check if transcription is available."""
        try:
            self._ensure_loaded()
            return True
        except Exception as e:
            print(f"[Transcriber] Model not available: {e}")
            return False
```

**python-src/audio/**init**.py:**
Add LocalTranscriber to exports:

```python
from .transcriber import LocalTranscriber
```

**python-src/main.py:**
Add transcriber initialization (lazy, similar to recording_session):

```python
from audio import LocalTranscriber
transcriber = None
```

Add transcribe_audio action handler:

- Parameters: audio_path, language (default "en")
- Initialize transcriber if None
- Call transcriber.transcribe(audio_path, language)
- Return {text, segments, language, duration}
- If transcription fails, return error with message

Add check_transcription action handler:

- Check if faster-whisper is available and model can be loaded
- Return {available: bool, model: string}
  </action>
  <verify>
  Run `cd python-src && python -c "from audio import LocalTranscriber; print('LocalTranscriber OK')"` - should print without errors (model download may happen on first use, not at import).
  </verify>
  <done>
  Python sidecar has faster-whisper transcriber. Accepts transcribe_audio action and returns structured transcript with segments.
  </done>
  </task>

<task type="auto">
  <name>Task 2: Create transcription service and complete audioRecordingService</name>
  <files>
    src/main/transcriptionService.ts
    src/main/audioRecordingService.ts
    src/main/index.ts
    src/main/preload.ts
  </files>
  <action>
**src/main/transcriptionService.ts (NEW):**
Create transcription job queue service:

```typescript
import { sendToPython } from "./pythonManager";
import { getDatabase } from "./database";
import { BrowserWindow } from "electron";
import * as fs from "fs";

interface TranscriptionJob {
  callRecordId: string;
  audioPath: string;
  projectId: string;
  candidateId: string;
}

const transcriptionQueue: TranscriptionJob[] = [];
let isProcessing = false;

export async function queueTranscription(job: TranscriptionJob): Promise<void> {
  // Update call record status to 'queued'
  const db = getDatabase();
  db.prepare(
    `
    UPDATE call_records SET transcription_status = 'queued' WHERE id = ?
  `,
  ).run(job.callRecordId);

  transcriptionQueue.push(job);
  processQueue();
}

async function processQueue(): Promise<void> {
  if (isProcessing || transcriptionQueue.length === 0) return;

  isProcessing = true;
  const job = transcriptionQueue.shift()!;

  try {
    const db = getDatabase();

    // Update status to processing
    db.prepare(
      `
      UPDATE call_records SET transcription_status = 'processing' WHERE id = ?
    `,
    ).run(job.callRecordId);

    // Call Python transcriber
    const result = (await sendToPython(
      {
        action: "transcribe_audio",
        audio_path: job.audioPath,
        language: "en",
      },
      300000,
    )) as { text: string; segments: any[]; duration: number };

    // Store transcript
    const transcriptId = crypto.randomUUID();
    const now = new Date().toISOString();

    db.prepare(
      `
      INSERT INTO transcripts (id, call_id, project_id, raw_text, segments_json, created_at)
      VALUES (?, ?, ?, ?, ?, ?)
    `,
    ).run(
      transcriptId,
      job.callRecordId,
      job.projectId,
      result.text,
      JSON.stringify(result.segments),
      now,
    );

    // Update call record
    db.prepare(
      `
      UPDATE call_records
      SET transcription_status = 'completed',
          duration_seconds = ?
      WHERE id = ?
    `,
    ).run(Math.round(result.duration), job.callRecordId);

    // Delete audio file to save space
    if (fs.existsSync(job.audioPath)) {
      fs.unlinkSync(job.audioPath);
    }

    // Notify renderer via toast
    const windows = BrowserWindow.getAllWindows();
    if (windows.length > 0) {
      windows[0].webContents.send("transcription-complete", {
        callRecordId: job.callRecordId,
        candidateId: job.candidateId,
      });
    }
  } catch (error) {
    const db = getDatabase();
    db.prepare(
      `
      UPDATE call_records
      SET transcription_status = 'failed',
          transcription_error = ?
      WHERE id = ?
    `,
    ).run(String(error), job.callRecordId);

    // Notify renderer of failure
    const windows = BrowserWindow.getAllWindows();
    if (windows.length > 0) {
      windows[0].webContents.send("transcription-failed", {
        callRecordId: job.callRecordId,
        candidateId: job.candidateId,
        error: String(error),
      });
    }
  }

  isProcessing = false;
  processQueue(); // Process next in queue
}

export function getTranscriptionStatus(callRecordId: string): string | null {
  const db = getDatabase();
  const row = db
    .prepare(
      `
    SELECT transcription_status FROM call_records WHERE id = ?
  `,
    )
    .get(callRecordId) as { transcription_status: string | null } | undefined;
  return row?.transcription_status ?? null;
}
```

**src/main/audioRecordingService.ts:**
Update attachRecording to call queueTranscription:

```typescript
import { queueTranscription } from "./transcriptionService";

// In attachRecording function, after creating call_record:
queueTranscription({
  callRecordId: callId,
  audioPath: currentSession.audioPath,
  projectId,
  candidateId,
});
```

**src/main/index.ts:**
Add IPC handlers for transcription status and events:

- "get-transcription-status" -> calls getTranscriptionStatus(callRecordId)
- Register listener for transcription-complete and transcription-failed events

**src/main/preload.ts:**
Add:

```typescript
getTranscriptionStatus: (callRecordId: string) =>
  ipcRenderer.invoke("get-transcription-status", callRecordId),
onTranscriptionComplete: (callback: (data: any) => void) =>
  ipcRenderer.on("transcription-complete", (_, data) => callback(data)),
onTranscriptionFailed: (callback: (data: any) => void) =>
  ipcRenderer.on("transcription-failed", (_, data) => callback(data)),
```

  </action>
  <verify>
Run `npm run typecheck` - should pass. Check that transcriptionService.ts compiles without errors.
  </verify>
  <done>
Transcription job queue processes recordings in background, updates database status, sends toast notifications to renderer, deletes audio files after successful transcription.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create floating RecordingPanel UI and recording store</name>
  <files>
    src/renderer/stores/recordingStore.ts
    src/renderer/components/recording/RecordingPanel.tsx
    src/renderer/components/recording/WaveformMeter.tsx
    src/renderer/components/recording/CandidateSelect.tsx
    src/renderer/components/wheel/ProjectLayout.tsx
    src/renderer/components/outreach/CallRecordCard.tsx
    src/renderer/components/outreach/CandidatePanel.tsx
  </files>
  <action>
**src/renderer/stores/recordingStore.ts (NEW):**
Create Zustand store for recording state:

```typescript
import { create } from "zustand";
import { toast } from "sonner";

type RecordingState = "idle" | "recording" | "stopped" | "attaching";

interface RecordingStore {
  state: RecordingState;
  isPanelExpanded: boolean;
  sessionId: string | null;
  startedAt: string | null;
  durationMs: number;
  micLevel: number;
  systemLevel: number;
  audioPath: string | null;

  // Actions
  startRecording: () => Promise<void>;
  stopRecording: () => Promise<void>;
  attachToCandidate: (candidateId: string, projectId: string) => Promise<void>;
  discardRecording: () => Promise<void>;
  togglePanel: () => void;
  setLevels: (mic: number, system: number) => void;
  updateDuration: () => void;
}

export const useRecordingStore = create<RecordingStore>((set, get) => ({
  state: "idle",
  isPanelExpanded: true,
  sessionId: null,
  startedAt: null,
  durationMs: 0,
  micLevel: 0,
  systemLevel: 0,
  audioPath: null,

  startRecording: async () => {
    try {
      const result = await window.api.startRecording();
      if (result.success) {
        set({
          state: "recording",
          sessionId: result.sessionId,
          startedAt: new Date().toISOString(),
          durationMs: 0,
        });
      } else {
        toast.error(result.error || "Failed to start recording");
      }
    } catch (error) {
      toast.error("Failed to start recording");
    }
  },

  stopRecording: async () => {
    try {
      const result = await window.api.stopRecording();
      if (result.success) {
        set({
          state: "stopped",
          durationMs: result.durationMs,
          audioPath: result.audioPath,
        });
      } else {
        toast.error(result.error || "Failed to stop recording");
      }
    } catch (error) {
      toast.error("Failed to stop recording");
    }
  },

  attachToCandidate: async (candidateId: string, projectId: string) => {
    try {
      set({ state: "attaching" });
      const result = await window.api.attachRecording(candidateId, projectId);
      if (result.success) {
        toast.success("Recording attached - transcription started");
        set({
          state: "idle",
          sessionId: null,
          startedAt: null,
          durationMs: 0,
          audioPath: null,
        });
      } else {
        set({ state: "stopped" });
        toast.error(result.error || "Failed to attach recording");
      }
    } catch (error) {
      set({ state: "stopped" });
      toast.error("Failed to attach recording");
    }
  },

  discardRecording: async () => {
    await window.api.discardRecording();
    set({
      state: "idle",
      sessionId: null,
      startedAt: null,
      durationMs: 0,
      audioPath: null,
    });
    toast.info("Recording discarded");
  },

  togglePanel: () => set((s) => ({ isPanelExpanded: !s.isPanelExpanded })),

  setLevels: (mic, system) => set({ micLevel: mic, systemLevel: system }),

  updateDuration: () => {
    const { startedAt, state } = get();
    if (state === "recording" && startedAt) {
      const now = new Date().getTime();
      const started = new Date(startedAt).getTime();
      set({ durationMs: now - started });
    }
  },
}));
```

**src/renderer/components/recording/WaveformMeter.tsx (NEW):**
Simple level meter component:

```typescript
import { cn } from "../../lib/utils";

interface WaveformMeterProps {
  level: number; // 0-1
  className?: string;
}

export function WaveformMeter({ level, className }: WaveformMeterProps) {
  const normalizedLevel = Math.min(1, Math.max(0, level));
  const percentage = normalizedLevel * 100;

  const getColor = () => {
    if (normalizedLevel > 0.8) return "bg-red-500";
    if (normalizedLevel > 0.5) return "bg-yellow-500";
    return "bg-green-500";
  };

  return (
    <div className={cn("flex-1 h-4 bg-muted rounded overflow-hidden", className)}>
      <div
        className={cn("h-full transition-all duration-75", getColor())}
        style={{ width: `${percentage}%` }}
      />
    </div>
  );
}
```

**src/renderer/components/recording/CandidateSelect.tsx (NEW):**
Candidate selector for attaching recording:

- Search input with candidate name filter
- Dropdown list of graduated candidates (outreach_status IS NOT NULL)
- Shows candidate name and match score
- onSelect callback triggers attachToCandidate

Use existing stores (useWorkflowStore for candidates) to get graduated candidates.

**src/renderer/components/recording/RecordingPanel.tsx (NEW):**
Floating draggable panel following RESEARCH.md pattern:

- Draggable via mouse events (position state, dragOffset ref)
- Minimized state shows only mic icon button with pulsing red dot if recording
- Expanded state shows:
  - Header with "Call Recording" title and minimize button
  - Level meters for System and Mic using WaveformMeter
  - Duration display (mm:ss format) when recording/stopped
  - Start/Stop/Discard buttons based on state
  - CandidateSelect when state is "stopped"
- Red pulsing dot indicator when recording is active
- useEffect with setInterval to update duration every 100ms when recording
- Listen for level updates from main process (via window.api event)

**src/renderer/components/wheel/ProjectLayout.tsx:**
Add RecordingPanel to the layout so it appears on all wheel sections:

```typescript
import { RecordingPanel } from "../recording/RecordingPanel";

// Inside the component, add after Outlet:
<RecordingPanel />
```

**src/renderer/components/outreach/CallRecordCard.tsx:**
Update to distinguish AI screening vs recruiter calls:

- Add `call.type` check ('screening' vs 'recruiter')
- For recruiter calls: show teal/cyan accent color instead of purple
- Add badge: "AI Screening" (purple) or "Recruiter Call" (teal)
- Show transcription status badge if type='recruiter' and transcription_status is 'queued' or 'processing'

**src/renderer/components/outreach/CandidatePanel.tsx:**
Update "Screening Calls" section to show all call types:

- Rename section to "Call Records" to include both types
- Existing callRecords loading already fetches all types
- CallRecordCard handles type distinction
  </action>
  <verify>
  Run `npm run typecheck` - should pass. Run `npm start` and verify RecordingPanel appears in bottom-right of project view. Click to expand/minimize.
  </verify>
  <done>
  Floating RecordingPanel component renders on all wheel sections. User can start/stop recording, see level meters, attach to candidate. CallRecordCard distinguishes AI vs Recruiter calls with different badge colors.
  </done>
  </task>

</tasks>

<verification>
1. TypeScript compiles: `npm run typecheck`
2. App starts: `npm start`
3. RecordingPanel visible: Check bottom-right corner when inside a project
4. Recording flow: Start -> Stop -> Select Candidate -> Attach
5. Transcription: After attachment, toast shows "transcription started", later "transcription complete"
6. Transcript visible: Open candidate panel, see "Recruiter Call" with teal badge
</verification>

<success_criteria>

- User can toggle system audio recording on/off with visible level meter confirming capture (REC-01)
- Recorded audio is transcribed locally via faster-whisper without blocking CV parsing (REC-02)
- Transcripts are attached to candidate record and visible in transcript viewer (REC-03)
- RecordingPanel is floating, draggable, accessible from any wheel section
- Recruiter calls distinguished from AI screening calls by color/badge
  </success_criteria>

<output>
After completion, create `.planning/phases/12-system-audio-transcription/12-02-SUMMARY.md`
</output>
