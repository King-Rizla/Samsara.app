# Phase 2.1: LLM Extraction - Research

**Researched:** 2026-01-24
**Domain:** Local LLM integration via Ollama for CV/resume structured data extraction
**Confidence:** HIGH (verified via official documentation, GitHub repos, and multiple technical sources)

## Summary

Phase 2.1 enhances the existing regex/NER extraction pipeline with local LLM (Ollama) for improved accuracy on semantic fields (work history, education, skills). Research confirms Ollama provides a mature Python API with structured JSON output support via Pydantic schemas, enabling type-safe extraction. The hybrid approach preserves the fast, reliable regex extraction for contact info (email/phone) while using LLM for context-dependent fields that benefit from semantic understanding.

Key findings:
- **Ollama Python library** provides both sync and async APIs with structured output via `format` parameter accepting Pydantic `model_json_schema()`
- **Qwen 2.5 7B** is the recommended model - optimized for structured data extraction, supports JSON mode, fast inference, and handles complex table/document structures
- **Structured outputs** use GBNF grammar constraints in llama.cpp to enforce JSON schema compliance, but validation is still recommended
- **Timeout configuration** is critical - 7B models require 60s minimum; keep model loaded with `keep_alive` to avoid cold-start delays
- **Hybrid extraction pattern** is well-established: regex for deterministic patterns (email, phone, URLs), LLM for semantic interpretation (job titles, company names, dates in context)

**Primary recommendation:** Use Ollama with Qwen 2.5 7B model, structured output via Pydantic schemas, and implement fallback to existing regex/NER extraction when LLM is unavailable or returns invalid data.

## Standard Stack

The established libraries/tools for this domain:

### Core

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| ollama | latest | Python client for Ollama API | Official library, supports structured outputs, async, error handling |
| pydantic | 2.x | JSON schema generation & validation | `model_json_schema()` generates Ollama-compatible schemas |
| Ollama server | 0.5+ | Local LLM inference | Structured output support added in v0.5, Windows native support |
| Qwen 2.5 7B | qwen2.5:7b | LLM model | Best-in-class for structured extraction, fast, 128K context |

### Supporting

| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| instructor | latest | Enhanced Pydantic integration | Optional - provides auto-retry, validation, timeout handling |
| httpx | (ollama dep) | HTTP client | Custom timeout configuration via Client kwargs |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Qwen 2.5 7B | Llama 3.1 8B | Llama slightly larger, Qwen better at structured output |
| Qwen 2.5 7B | Mistral 7B | Mistral faster but less accurate on complex schemas |
| Qwen 2.5 7B | Phi-3 3.8B | Phi much faster, lower accuracy on extraction |
| ollama native | instructor | Instructor adds retries/validation but extra dependency |

**Installation:**

```bash
# Add to python-src/requirements.txt
ollama>=0.4.0
pydantic>=2.0.0

# User must install Ollama separately:
# Windows: Download OllamaSetup.exe from ollama.ai
# Then: ollama pull qwen2.5:7b
```

## Architecture Patterns

### Recommended Project Structure

```
python-src/
├── main.py                    # IPC handler (existing)
├── extractors/
│   ├── contact.py             # Existing regex extraction (keep)
│   ├── sections.py            # Existing section detection (keep)
│   ├── work_history.py        # Add LLM extraction path
│   ├── education.py           # Add LLM extraction path
│   ├── skills.py              # Add LLM extraction path
│   └── llm/                   # NEW: LLM extraction module
│       ├── __init__.py
│       ├── client.py          # Ollama client wrapper
│       ├── schemas.py         # Pydantic models for LLM output
│       └── prompts.py         # Extraction prompts
├── schema/
│   └── cv_schema.py           # Existing TypedDict (keep)
└── requirements.txt
```

### Pattern 1: Ollama Client with Health Check

**What:** Wrapper around Ollama client with availability detection and timeout configuration
**When to use:** All LLM operations - ensures graceful degradation when Ollama unavailable

```python
# Source: https://github.com/ollama/ollama-python + verified patterns
from typing import Optional, Type, TypeVar
from pydantic import BaseModel
import ollama
from ollama import Client

T = TypeVar('T', bound=BaseModel)

class OllamaClient:
    """Wrapper for Ollama with health checks and structured output."""

    def __init__(
        self,
        model: str = "qwen2.5:7b",
        timeout: float = 60.0,
        keep_alive: str = "5m"
    ):
        self.model = model
        self.timeout = timeout
        self.keep_alive = keep_alive
        self._client: Optional[Client] = None
        self._available: Optional[bool] = None

    def is_available(self) -> bool:
        """Check if Ollama is running and model is available."""
        if self._available is not None:
            return self._available

        try:
            client = Client(timeout=5.0)  # Short timeout for health check
            models = client.list()
            model_names = [m.model for m in models.models]
            # Check if our model is available (with or without tag)
            base_model = self.model.split(':')[0]
            self._available = any(
                m.startswith(base_model) for m in model_names
            )
            self._client = Client(timeout=self.timeout)
        except Exception:
            self._available = False

        return self._available

    def extract(
        self,
        text: str,
        prompt: str,
        schema: Type[T],
        temperature: float = 0.0
    ) -> Optional[T]:
        """Extract structured data using LLM."""
        if not self.is_available():
            return None

        try:
            response = self._client.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text}
                ],
                format=schema.model_json_schema(),
                options={
                    "temperature": temperature,
                    "keep_alive": self.keep_alive
                }
            )
            return schema.model_validate_json(response.message.content)
        except Exception as e:
            # Log error, return None for fallback
            return None
```

### Pattern 2: Pydantic Schemas for LLM Output

**What:** Pydantic models that generate JSON schemas for Ollama structured output
**When to use:** Define expected LLM response structure with validation

```python
# Source: https://docs.ollama.com/capabilities/structured-outputs
from typing import List, Optional
from pydantic import BaseModel, Field


class LLMWorkEntry(BaseModel):
    """Work history entry extracted by LLM."""
    company: str = Field(description="Company or organization name")
    position: str = Field(description="Job title or role")
    start_date: Optional[str] = Field(
        None,
        description="Start date in format 'Month Year' or 'YYYY'"
    )
    end_date: Optional[str] = Field(
        None,
        description="End date or 'Present' if current role"
    )
    description: str = Field(
        default="",
        description="Brief description of responsibilities"
    )
    highlights: List[str] = Field(
        default_factory=list,
        description="Key achievements or bullet points"
    )


class LLMWorkHistory(BaseModel):
    """Complete work history extraction."""
    entries: List[LLMWorkEntry] = Field(
        default_factory=list,
        description="List of work experience entries in chronological order"
    )


class LLMEducationEntry(BaseModel):
    """Education entry extracted by LLM."""
    institution: str = Field(description="University, college, or school name")
    degree: str = Field(description="Degree type (BSc, MSc, PhD, etc.)")
    field_of_study: Optional[str] = Field(
        None,
        description="Subject or major field of study"
    )
    start_date: Optional[str] = Field(None, description="Start year")
    end_date: Optional[str] = Field(None, description="End/graduation year")
    grade: Optional[str] = Field(
        None,
        description="Grade, classification, or GPA (e.g., '2:1', 'First Class', '3.8')"
    )


class LLMEducation(BaseModel):
    """Complete education extraction."""
    entries: List[LLMEducationEntry] = Field(
        default_factory=list,
        description="List of education entries"
    )


class LLMSkillGroup(BaseModel):
    """Skill group with category."""
    category: str = Field(description="Category name as written by candidate")
    skills: List[str] = Field(description="Individual skills in this category")


class LLMSkills(BaseModel):
    """Skills extraction preserving candidate groupings."""
    groups: List[LLMSkillGroup] = Field(
        default_factory=list,
        description="Skill groups with categories as candidate organized them"
    )
```

### Pattern 3: Hybrid Extraction with Fallback

**What:** Try LLM extraction first, fall back to regex/NER if unavailable or invalid
**When to use:** All extraction paths - ensures reliability even when LLM fails

```python
# Source: Recommended pattern from research
from typing import List, Tuple
from schema.cv_schema import WorkEntry

def extract_work_history_hybrid(
    text: str,
    nlp,
    llm_client: OllamaClient
) -> Tuple[List[WorkEntry], str]:
    """
    Extract work history with LLM, fallback to regex.

    Returns:
        Tuple of (entries, extraction_method)
    """
    # Try LLM extraction first
    if llm_client.is_available():
        llm_result = llm_client.extract(
            text=text,
            prompt=WORK_HISTORY_PROMPT,
            schema=LLMWorkHistory,
            temperature=0.0
        )

        if llm_result and llm_result.entries:
            # Validate and convert to WorkEntry
            entries = []
            for llm_entry in llm_result.entries:
                entry = _llm_to_work_entry(llm_entry)
                if _validate_work_entry(entry):
                    entries.append(entry)

            if entries:
                return entries, "llm"

    # Fallback to existing regex/NER extraction
    from extractors.work_history import extract_work_history
    entries = extract_work_history(text, nlp)
    return entries, "regex"


def _llm_to_work_entry(llm_entry: LLMWorkEntry) -> WorkEntry:
    """Convert LLM output to schema WorkEntry."""
    from normalizers.dates import normalize_date

    return WorkEntry(
        company=llm_entry.company,
        position=llm_entry.position,
        start_date=normalize_date(llm_entry.start_date) if llm_entry.start_date else None,
        end_date=normalize_date(llm_entry.end_date) if llm_entry.end_date else None,
        description=llm_entry.description,
        highlights=llm_entry.highlights,
        confidence=0.85  # High confidence for LLM extraction
    )


def _validate_work_entry(entry: WorkEntry) -> bool:
    """Validate extracted work entry."""
    # Must have at least company or position
    if not entry.get('company') and not entry.get('position'):
        return False
    # Company/position shouldn't be too short
    if entry.get('company') and len(entry['company']) < 2:
        return False
    return True
```

### Pattern 4: Extraction Prompts

**What:** Carefully crafted prompts for CV extraction with structured output guidance
**When to use:** All LLM extraction calls

```python
# Source: Best practices from research
WORK_HISTORY_PROMPT = """You are a CV/resume parser. Extract work history entries from the provided text.

For each job entry, extract:
- company: The company or organization name (not the job title)
- position: The job title or role (not the company name)
- start_date: When they started (e.g., "January 2020", "2020", "Jan 2020")
- end_date: When they left, or "Present" if current
- description: Brief summary of the role
- highlights: Key achievements or responsibilities as bullet points

Guidelines:
- Extract only actual work experience, not education or skills
- Use British date interpretation (3/2/2020 = 3rd February)
- If dates use ranges like "2018 - 2020", extract both dates
- Preserve the candidate's original wording where possible
- If unsure about a field, omit it rather than guessing

Return as JSON only. Do not include any explanation or commentary."""

EDUCATION_PROMPT = """You are a CV/resume parser. Extract education entries from the provided text.

For each education entry, extract:
- institution: University, college, or school name
- degree: Type of qualification (BSc, BA, MSc, PhD, GCSE, A-Level, BTEC, etc.)
- field_of_study: Subject or major (e.g., "Computer Science", "Business Administration")
- start_date: Start year if provided
- end_date: Graduation year or expected graduation
- grade: Classification or GPA (e.g., "First Class", "2:1", "Distinction", "3.8 GPA")

Guidelines:
- Extract only education entries, not work experience
- Recognize UK qualifications: GCSEs, A-Levels, BTECs, HNDs, degree classifications
- For UK degrees, grades are typically: First, 2:1, 2:2, Third, Pass
- Include both university degrees and professional certifications

Return as JSON only. Do not include any explanation or commentary."""

SKILLS_PROMPT = """You are a CV/resume parser. Extract skills from the provided text.

Extract skills while PRESERVING the candidate's own groupings and categories.

For example, if the CV says:
  "Programming Languages: Python, JavaScript, Go
   Databases: PostgreSQL, MongoDB"

Return those exact groupings:
  [{"category": "Programming Languages", "skills": ["Python", "JavaScript", "Go"]},
   {"category": "Databases", "skills": ["PostgreSQL", "MongoDB"]}]

Guidelines:
- Use the candidate's exact category names, do not rename them
- If no categories are provided, use "Skills" as the default category
- Include both technical and soft skills
- Do not invent skills not mentioned in the text

Return as JSON only. Do not include any explanation or commentary."""
```

### Anti-Patterns to Avoid

- **Not checking Ollama availability:** Always check `is_available()` before extraction - Ollama may not be installed or model may not be pulled
- **Using default timeout:** Default 30s is too short for 7B models. Use 60s minimum.
- **Not setting temperature=0:** For extraction tasks, use temperature=0 for deterministic, consistent output
- **Skipping validation:** Always validate LLM output - structured output grammar doesn't guarantee semantic validity
- **Not implementing fallback:** If LLM fails, fall back to regex/NER extraction - never fail completely
- **Loading model per-request:** Use `keep_alive` to keep model loaded. First request is slow (model load), subsequent fast.
- **Not including JSON instruction in prompt:** Even with structured output, include "Return as JSON only" in prompt for best results
- **Expecting Qwen3 to work with JSON mode:** Qwen3 adds `<think>` tags that break JSON parsing. Use Qwen2.5 for structured output.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| JSON schema generation | Manual schema dict | Pydantic `model_json_schema()` | Handles complex types, validation, nested models |
| LLM response validation | Manual JSON parsing | Pydantic `model_validate_json()` | Type coercion, error messages, nested validation |
| Ollama availability check | HTTP ping | `ollama.list()` | Also verifies model availability |
| Retry on failure | Manual retry loop | instructor library | Exponential backoff, max retries, timeout |
| Date normalization | Custom parsing per format | Existing `normalize_date()` | Already handles British formats, edge cases |

**Key insight:** The existing regex/NER extractors are already written and tested. The LLM layer should enhance, not replace - add LLM as first-try with validated regex as fallback. This provides better extraction while maintaining reliability.

## Common Pitfalls

### Pitfall 1: Cold Start Delays

**What goes wrong:** First extraction takes 10-30 seconds, causing timeout or poor UX
**Why it happens:** Ollama must load model into GPU/CPU memory on first request
**How to avoid:**
1. "Warm up" model on sidecar startup with a simple prompt
2. Use `keep_alive` option to keep model loaded (default 5m)
3. Set appropriate timeout (60s for 7B model)
4. Consider background model loading while UI is idle

```python
def warmup_llm(client: OllamaClient) -> bool:
    """Warm up LLM model to avoid cold start delay."""
    if not client.is_available():
        return False

    try:
        # Simple prompt to load model
        client.extract(
            text="Hello",
            prompt="Reply with OK",
            schema=SimpleResponse,
            temperature=0.0
        )
        return True
    except Exception:
        return False
```
**Warning signs:** First CV takes 20+ seconds, subsequent take 2-3 seconds

### Pitfall 2: JSON Response Truncation

**What goes wrong:** LLM returns incomplete JSON, validation fails
**Why it happens:** Response exceeds max tokens, or network timeout
**How to avoid:**
1. Set adequate timeout (60s minimum for 7B)
2. Use `num_ctx` option if context needed
3. Validate response, fall back to regex on failure
4. Log truncation for debugging

```python
def safe_extract(client: OllamaClient, text: str, schema: Type[T]) -> Optional[T]:
    """Extract with truncation detection."""
    try:
        result = client.extract(text, prompt, schema)
        return result
    except ValidationError as e:
        # Log for debugging
        logger.warning(f"LLM response validation failed: {e}")
        return None
    except Exception as e:
        # Network/timeout error
        logger.warning(f"LLM extraction failed: {e}")
        return None
```
**Warning signs:** `ValidationError` on parse, incomplete entries, missing closing braces

### Pitfall 3: LLM Hallucination of Dates/Companies

**What goes wrong:** LLM invents plausible but wrong information
**Why it happens:** LLM generates statistically likely completions, not verified facts
**How to avoid:**
1. Use temperature=0 for deterministic output
2. Cross-validate with regex extraction (if both disagree, flag for review)
3. Prompt explicitly: "If unsure, omit rather than guess"
4. Validate date ranges make sense (start before end, not future dates)

```python
def validate_dates(entry: WorkEntry) -> bool:
    """Validate date range is sensible."""
    from dateutil.parser import parse
    from datetime import datetime

    try:
        if entry.get('start_date') and entry.get('end_date'):
            if entry['end_date'].lower() == 'present':
                return True
            start = parse(entry['start_date'], dayfirst=True)
            end = parse(entry['end_date'], dayfirst=True)
            # End must be after start
            if end < start:
                return False
            # Neither should be in future (with 1 year grace)
            if start > datetime.now() or end > datetime.now():
                return False
        return True
    except Exception:
        return True  # Can't validate, assume OK
```
**Warning signs:** Dates that don't match CV text, future dates, impossible ranges

### Pitfall 4: Qwen3 Thinking Tags Break JSON

**What goes wrong:** Response includes `<think>` tags, JSON parsing fails
**Why it happens:** Qwen3 models output reasoning traces before JSON
**How to avoid:**
1. Use Qwen2.5, not Qwen3, for structured output
2. If Qwen3 needed, strip `<think>...</think>` before parsing
3. Validate model name includes version

```python
RECOMMENDED_MODELS = [
    "qwen2.5:7b",      # Best for structured output
    "qwen2.5:14b",     # More capable, slower
    "llama3.1:8b",     # Good alternative
    "mistral:7b",      # Fastest, less accurate
]

# Avoid for structured output:
PROBLEMATIC_MODELS = [
    "qwen3",           # <think> tags break JSON
]
```
**Warning signs:** Error mentioning `<think>`, unexpected token errors

### Pitfall 5: Over-Reliance on LLM

**What goes wrong:** System fails when Ollama unavailable
**Why it happens:** No fallback to regex/NER extraction
**How to avoid:**
1. Always check `is_available()` first
2. Implement complete fallback to existing extractors
3. Track extraction method in response for debugging
4. Log LLM failures but don't fail extraction

```python
def extract_with_fallback(text: str, nlp, llm_client) -> Tuple[Result, str]:
    """
    Extract with LLM, fallback to regex.

    Returns tuple of (result, method: "llm" | "regex" | "none")
    """
    method = "none"

    # Try LLM
    if llm_client.is_available():
        llm_result = llm_client.extract(text, ...)
        if llm_result and validate(llm_result):
            return llm_result, "llm"

    # Fallback to regex
    regex_result = extract_with_regex(text, nlp)
    if regex_result:
        return regex_result, "regex"

    return empty_result(), "none"
```
**Warning signs:** Extraction failures on machines without Ollama, CI/CD failures

## Code Examples

Verified patterns from official sources:

### Ollama Structured Output with Pydantic

```python
# Source: https://docs.ollama.com/capabilities/structured-outputs
from ollama import chat
from pydantic import BaseModel
from typing import List, Optional


class WorkEntry(BaseModel):
    company: str
    position: str
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    description: str = ""
    highlights: List[str] = []


class WorkHistory(BaseModel):
    entries: List[WorkEntry] = []


def extract_work_history_llm(text: str, model: str = "qwen2.5:7b") -> WorkHistory:
    """Extract work history using Ollama with structured output."""
    response = chat(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "Extract work history entries from the CV text. Return as JSON."
            },
            {"role": "user", "content": text}
        ],
        format=WorkHistory.model_json_schema(),
        options={
            "temperature": 0,
            "keep_alive": "5m"
        }
    )

    return WorkHistory.model_validate_json(response.message.content)
```

### Checking Ollama Availability

```python
# Source: https://github.com/ollama/ollama-python
import ollama
from ollama import ResponseError


def check_ollama_ready(model: str = "qwen2.5:7b") -> tuple[bool, str]:
    """
    Check if Ollama is running and model is available.

    Returns:
        Tuple of (is_ready, message)
    """
    try:
        # Check if server is responding
        models = ollama.list()

        # Check if specific model is available
        model_names = [m.model for m in models.models]
        base_model = model.split(':')[0]

        for m in model_names:
            if m.startswith(base_model):
                return True, f"Ollama ready with {m}"

        return False, f"Model {model} not found. Run: ollama pull {model}"

    except ResponseError as e:
        return False, f"Ollama error: {e.error}"
    except Exception as e:
        return False, f"Ollama not running: {str(e)}"
```

### Client with Timeout Configuration

```python
# Source: https://deepwiki.com/ollama/ollama-python/5.2-configuration-and-options
from ollama import Client


def create_ollama_client(
    timeout: float = 60.0,
    host: str = "http://localhost:11434"
) -> Client:
    """
    Create Ollama client with custom timeout.

    Args:
        timeout: Request timeout in seconds (60s recommended for 7B models)
        host: Ollama server URL
    """
    return Client(
        host=host,
        timeout=timeout  # Passed to httpx.Client
    )


# Usage
client = create_ollama_client(timeout=60.0)
response = client.chat(
    model="qwen2.5:7b",
    messages=[...],
    format=schema,
    options={"keep_alive": "10m"}  # Keep model loaded
)
```

### Integration with Existing Extractors

```python
# Source: Recommended pattern for hybrid extraction
from typing import List, Tuple
from extractors.work_history import extract_work_history as extract_regex
from extractors.llm.client import OllamaClient
from extractors.llm.schemas import LLMWorkHistory
from extractors.llm.prompts import WORK_HISTORY_PROMPT
from schema.cv_schema import WorkEntry
from normalizers.dates import normalize_date


def extract_work_history(
    text: str,
    nlp,
    llm_client: OllamaClient = None
) -> Tuple[List[WorkEntry], dict]:
    """
    Extract work history with hybrid approach.

    Args:
        text: Work experience section text
        nlp: Loaded spaCy model
        llm_client: Optional Ollama client

    Returns:
        Tuple of (entries, metadata with method/confidence)
    """
    metadata = {"method": "regex", "llm_available": False}

    # Try LLM extraction
    if llm_client and llm_client.is_available():
        metadata["llm_available"] = True

        llm_result = llm_client.extract(
            text=text,
            prompt=WORK_HISTORY_PROMPT,
            schema=LLMWorkHistory,
            temperature=0.0
        )

        if llm_result and llm_result.entries:
            entries = []
            for e in llm_result.entries:
                entry = WorkEntry(
                    company=e.company,
                    position=e.position,
                    start_date=normalize_date(e.start_date) if e.start_date else None,
                    end_date=normalize_date(e.end_date) if e.end_date else None,
                    description=e.description,
                    highlights=e.highlights,
                    confidence=0.85  # LLM extraction
                )
                entries.append(entry)

            if entries:
                metadata["method"] = "llm"
                return entries, metadata

    # Fallback to regex/NER
    entries = extract_regex(text, nlp)
    return entries, metadata
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Regex-only extraction | Hybrid LLM + regex | 2025 | Better accuracy on semantic fields |
| Generic JSON mode | JSON Schema structured output | Ollama 0.5 (2024) | Guaranteed schema compliance |
| Cloud LLM APIs | Local Ollama | 2024+ | Privacy, no API costs, offline |
| GPT-3.5/4 for extraction | Qwen 2.5 / Llama 3.1 | 2025 | Comparable quality, local execution |
| Manual prompt engineering | Pydantic schema in prompt | 2025 | Type-safe, validated output |

**Deprecated/outdated:**
- Qwen3 for structured output: Use Qwen2.5 - Qwen3's thinking tags break JSON parsing
- Default 30s timeout: Too short for 7B models, use 60s+
- Loading model per-request: Use `keep_alive` to avoid cold start delays

## Open Questions

Things that couldn't be fully resolved:

1. **Optimal model size vs speed tradeoff**
   - What we know: 7B models run in 2-5s on decent hardware, 14B in 5-10s
   - What's unclear: User hardware varies widely; may need configuration option
   - Recommendation: Default to qwen2.5:7b, document hardware requirements, allow user override

2. **Error recovery when model partially loaded**
   - What we know: Model loading can fail mid-way, leaving Ollama in inconsistent state
   - What's unclear: Best recovery strategy beyond retry
   - Recommendation: Implement health check with model-specific query, timeout and retry with fallback

3. **Extraction accuracy on multi-column CVs**
   - What we know: LLM receives linearized text, may lose column relationships
   - What's unclear: Whether LLM handles this better or worse than regex
   - Recommendation: Test with multi-column CVs during Phase 2.1, adjust prompts if needed

4. **Bundling Ollama with Electron app**
   - What we know: Ollama must be installed separately by user
   - What's unclear: Whether to auto-detect, auto-install, or just document
   - Recommendation: Detect availability, show helpful message if missing, don't auto-install

## Sources

### Primary (HIGH confidence)
- [Ollama Python Library](https://github.com/ollama/ollama-python) - API usage, Client class, error handling
- [Ollama Structured Outputs](https://docs.ollama.com/capabilities/structured-outputs) - JSON schema format, Pydantic integration
- [Ollama Windows Installation](https://docs.ollama.com/windows) - Setup, availability check
- [Qwen 2.5 on Ollama](https://ollama.com/library/qwen2.5) - Model capabilities, structured output support
- [Ollama Configuration](https://deepwiki.com/ollama/ollama-python/5.2-configuration-and-options) - Timeout, keep_alive settings

### Secondary (MEDIUM confidence)
- [Best LLMs for Ollama 2026](https://visionvix.com/best-llm-for-ollama/) - Model comparison for extraction tasks
- [Instructor with Ollama](https://python.useinstructor.com/integrations/ollama/) - Enhanced Pydantic integration
- [Ollama Models Comparison](https://collabnix.com/best-ollama-models-in-2025-complete-performance-comparison/) - Performance benchmarks
- [Resume Parsing with LLMs](https://www.datumo.io/blog/parsing-resumes-with-llms-a-guide-to-structuring-cvs-for-hr-automation) - Hybrid extraction patterns

### Tertiary (LOW confidence)
- [Ollama JSON Issues](https://github.com/ollama/ollama/issues/3154) - Performance with format=json
- [Ollama Timeout Issues](https://github.com/ollama/ollama/issues/4967) - Truncation detection needs
- [LLM Hallucination 2026](https://blogs.library.duke.edu/blog/2026/01/05/its-2026-why-are-llms-still-hallucinating/) - Hallucination rates in extraction

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Ollama Python library verified, Pydantic integration documented
- Architecture: HIGH - Patterns from official docs and proven community usage
- Pitfalls: HIGH - Verified via GitHub issues, official documentation
- Model recommendations: MEDIUM - Benchmarks vary, but Qwen 2.5 consistently recommended

**Research date:** 2026-01-24
**Valid until:** 2026-02-24 (30 days - Ollama updates frequently, check for new model versions)
